{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Objective: \n",
    "    Set parameters to initial values\n",
    "    \n",
    "    Arguments:\n",
    "    layer_dims -- List containing the dimension of each layer in the neural network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Dictionary of parameters \"W1\", \"b1\", \"W2\", \"b2\", ..., \"WL\", \"bL\"\n",
    "        Wl -- Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        bl -- Bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1/layer_dims[l-1])\n",
    "        #Xavier's approach\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Objective: \n",
    "    Implement the forward propagation linear part of a layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Activations from previous layer of shape (number of units of previous layer, sample size)\n",
    "    W -- Weights matrix of shape (number of units of current layer, number of units of previous layer)\n",
    "    b -- Bias vector of shape (number of units of current layer, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- Pre-activation value of layer\n",
    "    cache -- Information passed through in this function to be used in backward propagation step\n",
    "    \"\"\"\n",
    "    Z = W @ A_prev + b\n",
    "    \n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A_prev, W, b, activation_function):\n",
    "    \"\"\"\n",
    "    Objective: \n",
    "    Implement the forward propagation activation part of a layer l\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Activations from previous layer of shape (number of units of previous layer, sample size)\n",
    "    W -- Weights matrix of shape (number of units of current layer, number of units of previous layer)\n",
    "    b -- Bias vector of shape (number of units of current layer, 1)\n",
    "    activation_function -- The type of function to be used. Stored as a string. \n",
    "                           Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                           Any other string will output a warning and default to a linear function\n",
    "    \n",
    "    Returns:\n",
    "    A -- Activation value of layer\n",
    "    cache -- Information passed through in this function to be used in backward propagation step. Has a linear\n",
    "    component and an actviation component\n",
    "    \"\"\"\n",
    "    if activation_function == \"relu\":\n",
    "        Z, linear_cache = forward_linear(A_prev, W, b)\n",
    "        A = Z * (Z > 0)\n",
    "        activation_cache = Z\n",
    "    elif activation_function == \"sigmoid\":\n",
    "        Z, linear_cache = forward_linear(A_prev, W, b)\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        activation_cache = Z\n",
    "    elif activation_function == \"tanh\":\n",
    "        Z, linear_cache = forward_linear(A_prev, W, b)\n",
    "        A = np.tanh(Z)\n",
    "        activation_cache = Z\n",
    "    else:\n",
    "        print(\"***Warning: Error in activation_function input. Relaying a linear output***\")\n",
    "        Z, linear_cache = forward_linear(A_prev, W, b)\n",
    "        A = Z\n",
    "        activation_cache = Z\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters, activation_function_layer, activation_function_output):\n",
    "    \"\"\"\n",
    "    Objective: \n",
    "    Implement forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Data matrix of shape (number of features, sample size)\n",
    "    parameters -- Output of initialize_parameters(layer_dims)\n",
    "    activation_function_layer -- Type of activation function hidden layers take.\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    activation_function_output -- Type of function output layer takes\n",
    "                                  Currently \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                  Any other string will output a warning and default to a linear function\n",
    "    \n",
    "    Returns:\n",
    "    AL -- Last activation value of shape (1, sample size)\n",
    "    caches -- List of all caches containing every cache from forward_activation(A_prev, W, b, activation_function).\n",
    "    There are L of them indexed from 0 to L-1\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 #Number of layers of neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        A, cache = forward_activation(A_prev, W, b, activation_function_layer)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    W = parameters[\"W\" + str(L)]\n",
    "    b = parameters[\"b\" + str(L)]\n",
    "    AL, cache = forward_activation(A, W, b, activation_function_output)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, cost_function):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Implement cost function\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- Last activation value of shape (1, sample size)\n",
    "    Y -- True value of shape (1, sample size)\n",
    "    cost_function -- The type of function to be used. Stored as a string. \n",
    "                     Currently, \"cross_entropy\" or \"diff_squared\" are accepted. \n",
    "                     Any other string will output a warning and default to a \"diff_squared\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    if cost_function == \"cross_entropy\":\n",
    "        cost = -1/m * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    elif cost_function == \"diff_squared\":\n",
    "        cost = 1/m * 1/2 * np.sum((Y-AL)**2)\n",
    "    else:\n",
    "        print(\"***Warning: Error in cost_function input. Using a difference squared cost function***\")\n",
    "        cost = 1/m * 1/2 * np.sum((Y-AL)**2)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_linear(dZ, cache):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Implement the backward propagation linear part of a layer l\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- Gradient of cost with respect to pre-activation value of current layer l of \n",
    "          shape(number of units in current layer, sample size)\n",
    "    cache -- Tuple of values containing (A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of cost with respect to activation value of previous layer l-1\n",
    "    dW -- Gradient of cost with respect to W of current layer l\n",
    "    db -- Gradient of cost with respect to b of current layer l\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    dW = 1/m * dZ @ A_prev.T\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA, cache, activation_function):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Implements the backward propagation activation part of a layer l\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- Gradient of cost with respect to activation value of current layer l of\n",
    "          shape(number of units in current layer, sample size)\n",
    "    cache -- Tuple of values containing (linear_cache, activation_cache) for layer l\n",
    "    activation_function -- The type of function to be used. Stored as a string.\n",
    "                           Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                           Any other string will output a warning and default to a linear function\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of cost with respect to activation value of previous layer l-1\n",
    "    dW -- Gradient of cost with respect to weights matrix of current layer l\n",
    "    db -- Gradient of cost with respect to bias vector of current layer l\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation_function == \"relu\":\n",
    "        dG = (activation_cache > 0)\n",
    "        dZ = dA * dG\n",
    "        dA_prev, dW, db = backward_linear(dZ, linear_cache)\n",
    "    elif activation_function == \"sigmoid\":\n",
    "        G = 1/(1 + np.exp(-activation_cache))\n",
    "        dG = G * (1 - G)\n",
    "        dZ = dA * dG\n",
    "        dA_prev, dW, db = backward_linear(dZ, linear_cache)\n",
    "    elif activation_function == \"tanh\":\n",
    "        dG = 1 / np.cosh(activation_cache)**2\n",
    "        dZ = dA * dG\n",
    "        dA_prev, dW, db = backward_linear(dZ, linear_cache)\n",
    "    else:\n",
    "        print(\"***Error in activation_function input. Relaying a linear output***\")\n",
    "        dG = np.ones(activation_cache.shape)\n",
    "        dZ = dA * dG\n",
    "        dA_prev, dW, db = backward_linear(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y, caches, cost_function, activation_function_layer, activation_function_output,\\\n",
    "                  parameters, learning_rate):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Implement backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- Last activation value of shape (1, sample size)\n",
    "    Y -- True value of shape (1, sample size)\n",
    "    caches -- List of all caches containing every cache from forward_activation(A_prev, W, b, activation_function).\n",
    "    There are L of them indexed from 0 to L-1\n",
    "    cost_function -- The type of function to be used\n",
    "                     Currently, \"cross_entropy\" or \"diff_squared\" are accepted. \n",
    "                     Any other string will output a warning and default to a \"diff_squared\" function\n",
    "    activation_function_layer -- Type of activation function hidden layers take.\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    activation_function_output -- Type of function output layer takes\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    \n",
    "    parameters -- Currently set parameter values that will be updated. \n",
    "                  Dictionary containing values for \"W1\", \"b1\", \"W2\", \"b2\", ..., \"WL\", \"bL\"\n",
    "    learning_rate -- Hyperparameter positive real number used to dictate how fast gradient descent goes\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Updated set of parameter values.\n",
    "                  Dictionary containing values for \"W1\", \"b1\", \"W2\", \"b2\", ..., \"WL\", \"bL\"\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) #Number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) #Ensuring that Y and AL have the same shape\n",
    "    \n",
    "    #Calculating dAL\n",
    "    if cost_function == \"cross_entropy\":\n",
    "        dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    elif cost_function == \"diff_squared\":\n",
    "        dAL = -(Y-AL)\n",
    "    else:\n",
    "        print(\"***Error in cost_function input. Using a difference squared cost function***\")\n",
    "        dAL = -(Y-AL)\n",
    "    \n",
    "    #Storing grads in layer L\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev, dW, db = backward_activation(dAL, current_cache, activation_function_output)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db\n",
    "    \n",
    "    parameters[\"W\" + str(L)] = parameters[\"W\" + str(L)] - learning_rate*grads[\"dW\" + str(L)]\n",
    "    parameters[\"b\" + str(L)] = parameters[\"b\" + str(L)] - learning_rate*grads[\"db\" + str(L)]\n",
    "    #Storing grads in layers L-1, ..., 1\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dAl = grads[\"dA\" + str(l+1)]\n",
    "        dA_prev, dW, db = backward_activation(dAl, current_cache, activation_function_layer)\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = dA_prev, dW, db\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_calibrate(X_train, Y_train, layers_dims, learning_rate, num_iterations, print_cost_flag, \\\n",
    "                       cost_function, activation_function_layer, activation_function_output):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Calibrate a neural network\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- Input training data of shape (number of features, sample size)\n",
    "    Y_train -- Output training data of shape (1, sample size)\n",
    "    layers_dims -- -- List containing the dimension of each layer in the neural network\n",
    "    learning_rate -- Hyperparameter positive real number used to dictate how fast gradient descent goes\n",
    "    num_iterations -- Number of iterations to go over training dataset\n",
    "    print_cost_flag -- Boolean flag that specifies whether or not cost should be printed and graphed\n",
    "    cost_function -- The type of function to be used\n",
    "                     Currently, \"cross_entropy\" or \"diff_squared\" are accepted. \n",
    "                     Any other string will output a warning and default to a \"diff_squared\" function\n",
    "    activation_function_layer -- Type of activation function hidden layers take.\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    activation_function_output -- Type of function output layer takes\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Dictionary of calibrated parameters \"W1\", \"b1\", \"W2\", \"b2\", ..., \"WL\", \"bL\"\n",
    "                  Wl -- Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                  bl -- Bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_prop(X_train, parameters, activation_function_layer, activation_function_output)\n",
    "        \n",
    "        cost = compute_cost(AL, Y_train, cost_function)\n",
    "        \n",
    "        parameters = backward_prop(AL, Y_train, caches, cost_function, activation_function_layer,\\\n",
    "                                   activation_function_output, parameters, learning_rate)\n",
    "        \n",
    "        if print_cost_flag and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost_flag and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "    if print_cost_flag:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.xlabel(\"# iterations (tens)\")\n",
    "        plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(X, parameters, activation_function_layer, activation_function_output, binary_flag):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Estimate output given input and model specifications\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input data of shape(number of features, 1)\n",
    "    parameters -- Dictionary of calibrated parameters \"W1\", \"b1\", \"W2\", \"b2\", ..., \"WL\", \"bL\"\n",
    "                  Wl -- Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                  bl -- Bias vector of shape (layer_dims[l], 1)\n",
    "    activation_function_layer -- Type of activation function hidden layers take.\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    activation_function_output -- Type of function output layer takes\n",
    "                                 Currently, \"relu\", \"sigmoid\", or \"tanh\" are accepted. \n",
    "                                 Any other string will output a warning and default to a linear function\n",
    "    binary_flag -- Boolean variable that indicates whether or not the output is binary\n",
    "    Returns:\n",
    "    AL -- Estimate value of Y\n",
    "    \"\"\"\n",
    "    AL, _ = forward_prop(X, parameters, activation_function_layer, activation_function_output)\n",
    "    \n",
    "    if binary_flag:\n",
    "        AL = (AL >= 0.5) * 1\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters =  {'W1': array([[1.98600498, 1.86257879]]), 'b1': array([[-3.16227886]])}\n",
      "Y00 =  [[0]]\n",
      "Y01 =  [[0]]\n",
      "Y10 =  [[0]]\n",
      "Y11 =  [[1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AND Model\"\"\"\n",
    "\n",
    "X_train = np.array([[0,0,1,1],[0,1,0,1]])\n",
    "Y_train = np.array([[0,0,0,1]])\n",
    "layers_dims = [2, 1]\n",
    "learning_rate = 0.005\n",
    "num_iterations = 10000\n",
    "print_cost_flag = False\n",
    "cost_function = \"cross_entropy\"\n",
    "activation_function_layer = \"relu\"\n",
    "activation_function_output = \"sigmoid\"\n",
    "\n",
    "parameters = NN_model_calibrate(X_train, Y_train, layers_dims, learning_rate, num_iterations, print_cost_flag, \\\n",
    "                                cost_function, activation_function_layer, activation_function_output)\n",
    "print(\"Parameters = \", parameters)\n",
    "\n",
    "binary_flag = True\n",
    "X00 = np.array([[0],[0]])\n",
    "Y00 = NN_predict(X00, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y00 = \", Y00)\n",
    "\n",
    "X01 = np.array([[0],[1]])\n",
    "Y01 = NN_predict(X01, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y01 = \", Y01)\n",
    "\n",
    "X10 = np.array([[0],[1]])\n",
    "Y10 = NN_predict(X10, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y10 = \", Y10)\n",
    "\n",
    "X11 = np.array([[1],[1]])\n",
    "Y11 = NN_predict(X11, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y11 = \", Y11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters =  {'W1': array([[2.92377638, 2.91020721]]), 'b1': array([[-0.84222213]])}\n",
      "Y00 =  [[0]]\n",
      "Y01 =  [[1]]\n",
      "Y10 =  [[1]]\n",
      "Y11 =  [[1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"OR Model\"\"\"\n",
    "\n",
    "X_train = np.array([[0,0,1,1],[0,1,0,1]])\n",
    "Y_train = np.array([[0,1,1,1]])\n",
    "layers_dims = [2, 1]\n",
    "learning_rate = 0.005\n",
    "num_iterations = 10000\n",
    "print_cost_flag = False\n",
    "cost_function = \"cross_entropy\"\n",
    "activation_function_layer = \"relu\"\n",
    "activation_function_output = \"sigmoid\"\n",
    "\n",
    "parameters = NN_model_calibrate(X_train, Y_train, layers_dims, learning_rate, num_iterations, print_cost_flag, \\\n",
    "                                cost_function, activation_function_layer, activation_function_output)\n",
    "print(\"Parameters = \", parameters)\n",
    "\n",
    "binary_flag = True\n",
    "X00 = np.array([[0],[0]])\n",
    "Y00 = NN_predict(X00, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y00 = \", Y00)\n",
    "\n",
    "X01 = np.array([[0],[1]])\n",
    "Y01 = NN_predict(X01, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y01 = \", Y01)\n",
    "\n",
    "X10 = np.array([[0],[1]])\n",
    "Y10 = NN_predict(X10, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y10 = \", Y10)\n",
    "\n",
    "X11 = np.array([[1],[1]])\n",
    "Y11 = NN_predict(X11, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y11 = \", Y11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters =  {'W1': array([[-4.48145367]]), 'b1': array([[2.01123232]])}\n",
      "Y0 =  [[1]]\n",
      "Y1 =  [[0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NOT Model\"\"\"\n",
    "\n",
    "X_train = np.array([[0,1]])\n",
    "Y_train = np.array([[1,0]])\n",
    "layers_dims = [1, 1]\n",
    "learning_rate = 0.005\n",
    "num_iterations = 10000\n",
    "print_cost_flag = False\n",
    "cost_function = \"cross_entropy\"\n",
    "activation_function_layer = \"relu\"\n",
    "activation_function_output = \"sigmoid\"\n",
    "\n",
    "parameters = NN_model_calibrate(X_train, Y_train, layers_dims, learning_rate, num_iterations, print_cost_flag, \\\n",
    "                                cost_function, activation_function_layer, activation_function_output)\n",
    "print(\"Parameters = \", parameters)\n",
    "\n",
    "binary_flag = True\n",
    "X0 = np.array([[0]])\n",
    "Y0 = NN_predict(X0, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y0 = \", Y0)\n",
    "\n",
    "X1 = np.array([[1]])\n",
    "Y1 = NN_predict(X1, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y1 = \", Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters =  {'W1': array([[ 1.6997553 , -1.69943111],\n",
      "       [ 1.33783496,  1.33788737],\n",
      "       [ 0.72726993,  0.48364274],\n",
      "       [-1.70868187,  1.70909907]]), 'b1': array([[-2.08283875e-04],\n",
      "       [-1.33809773e+00],\n",
      "       [ 6.13245262e-05],\n",
      "       [-1.18670461e-04]]), 'W2': array([[ 2.17039304, -2.36977695,  0.85319293,  2.29228928]]), 'b2': array([[-1.37371298]])}\n",
      "Y00 =  [[0]]\n",
      "Y01 =  [[1]]\n",
      "Y10 =  [[1]]\n",
      "Y11 =  [[0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"XOR Model\"\"\"\n",
    "\n",
    "X_train = np.array([[0,0,1,1],[0,1,0,1]])\n",
    "Y_train = np.array([[0,1,1,0]])\n",
    "layers_dims = [2, 4, 1]\n",
    "learning_rate = 0.005\n",
    "num_iterations = 10000\n",
    "print_cost_flag = False\n",
    "cost_function = \"cross_entropy\"\n",
    "activation_function_layer = \"relu\"\n",
    "activation_function_output = \"sigmoid\"\n",
    "\n",
    "parameters = NN_model_calibrate(X_train, Y_train, layers_dims, learning_rate, num_iterations, print_cost_flag, \\\n",
    "                                cost_function, activation_function_layer, activation_function_output)\n",
    "print(\"Parameters = \", parameters)\n",
    "\n",
    "binary_flag = True\n",
    "X00 = np.array([[0],[0]])\n",
    "Y00 = NN_predict(X00, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y00 = \", Y00)\n",
    "\n",
    "X01 = np.array([[0],[1]])\n",
    "Y01 = NN_predict(X01, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y01 = \", Y01)\n",
    "\n",
    "X10 = np.array([[0],[1]])\n",
    "Y10 = NN_predict(X10, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y10 = \", Y10)\n",
    "\n",
    "X11 = np.array([[1],[1]])\n",
    "Y11 = NN_predict(X11, parameters, activation_function_layer, activation_function_output, binary_flag)\n",
    "print(\"Y11 = \", Y11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
