{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Organization & Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document aims to provide the reader with a detailed description of my implementation of an NLP sentiment classifier project. The underlying objective of this assignment is to build a text classification model that would predict a label value for word sentences (aka comments) given in the csv file \"id_label.csv\". Specifically, I am required to build a model that would predict the label values of comments 4000 through 5571.\n",
    "\n",
    "I am provided with the following pieces of information:  \n",
    "1. The data file \"corpus.txt\" providing the corpus of comments and their words to be used in this assignment\n",
    "2. The data file \"id_label.csv\" providing the list of comments and their corresponding label values (which are binary in this case, taking either the value zero or one). Comments 0 through 3999 are labelled (i.e., have a label value), comments 4000 through 5571 are not and need to be estimated\n",
    "\n",
    "The rest of this document is organized as follows in the order given below:\n",
    "1. Executive summary: Used for providing the gist of the analysis and results obtained giving the reader a sense of the fruits of the work and journey taken to arrive at the results\n",
    "2. High level approach: Used for providing the reader with the overall strategy I took in order to arrive at the final results. A benefit of having such a strategy in mind is its possible generalizations to tasks beyond this project\n",
    "3. Implementation: The heart of the project. This section contains the following subsections:    \n",
    "..a. Assumptions & objective setting: This section provides the main assumptions I make from the information given as well as in the approach that will be taken. This section formalizes the assignment requirement into a mathematical problem. This includes the key performance indicators (KPIs) that will be used in selecting the best model to make the final predictions  \n",
    "..b. Data management module: This section provides the process of reading and parsing the data, as well as the logic used in handling any existing anomalies (e.g., missing data, erroneous data)  \n",
    "..c. Modelling module: This section provides the details of the models used, including model performance, the final model I recommend to be used based on certain criteria. All predictions are exported into csv files starting with the term \"labels_predict\" and then a short-hand name of the model followed by \".csv\".\n",
    "4. Future improvements: This section provides some ideas on how the approach detailed in this document can be improved and generalized given more information and resources\n",
    "5. Appendix: Acknowledgements & key references: This section provides acknowledgements to key individuals who helped me in certain aspects of this project, as well as references that made implementing this project possible\n",
    "\n",
    "Finally, I would like to point out that the style of this document is meant to be modular across sections and sub-sections, and linear within these sections and sub-sections as to guide the reader on the thought process of implementing this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem presented is identified as a binary classification problem with features represented as arrays of strings. When parsing the data given, I used a \"latin1\" type of encoding as the default \"utf-8\" did not work. This is due to the presence of non-ASCII characters in the corpus dataset. After reading in the data set, I identified two comments missing from the corpus: \"3372\" and \"4822\". Because the first comment, \"3372\" is labelled, I decided to ignore using it in model development. As for the second comment \"4822\", since I do not have any information on what words contain it, I decided to use a Bayesian base-rate approach: use a random number generator with probability of success equal to the proportion of \"1\" labels divided by all labelled comments used in training, validation, and testing.\n",
    "\n",
    "A key assumption I made when analyzing the labelled dataset is that the label value that is less common (\"1\" in this case) is the one of greater interest. Thus, I chose to use the f1 score as the key metric I am trying to maximize. Accuracy is used as a minimum requirement for filtering out models by comparing accuracy against the accuracy of a baseline stragegy: setting all values of the test dataset with the more common label (\"0\" in this case) and calculating accuracy (this was about 86% across the samples I used, which means prediction accuracy using the same smaple must surpass this amount).\n",
    "\n",
    "For modelling, the labelled dataset (excluding those that do not have words in the corpus dataset, which is comment \"3372\" in this case) is randomly split into three parts: training (for training model parameters), validation (for training model hyperparameters that maximize the f1 score, while meeting minimum accuracy requirements; for models that do not require hyperparameter calibration, this dataset is simply merged with the training dataset), and a testing dataset (which will be used in assessing model performance of each model used). I adoped an iterative approach, starting from simple (e.g., word order not important and independent of other words), quick-to-implement models, to complex (e.g., word read sequentially) models. The following list provides the details of the models I used and the results I obtained:\n",
    "1. Models that use feature representation represented by word count:  \n",
    "..a. Naive Bayes: Accuracy=98.8%, f1=95.3% on the testing dataset  \n",
    "..b. Linear regression with elastic net regularization: Accuracy=98.8%, f1=95.15% on the testing dataset\n",
    "2. Models that take in words sequentially:\n",
    "..a. Standard L-layered neural network: Accuracy=91.6%, f1=68.4% (varies a by about 5% per attempt) on testing dataset\n",
    "..b. Recurrent neural network (RNN): Accuracy=99.3%, f1=97.1% on testing dataset\n",
    "A similarity comparison was made between the top three models (RNN, Naive Bayes, and elastic net) on the predictions of the unlabelled comments showing very consistent predictions (all three models are about within a two percent difference from each other). \n",
    "\n",
    "Thus, given this iteration of model exploration, I would opt to choose the Naive Bayes results if simplicity and large scalability is the focus (assuming the accuracy comparison still holds if we use larger dataset). However, if computational power nor scalability are an issue, then we simply pick the most accurate model.\n",
    "\n",
    "Potential future improvements on this project could be the following items:\n",
    "1. Conducting further parameter and hyperparameter searches on the model (depending on resources available). This includes improving the hyperparameter optimization process (e.g., via more iterations especially if computational power is available, via a more effective method than randomization such as gradient ascent and coarse-to-fine methods)\n",
    "2. Exploring a larger range of models to determine if any would be predictive (e.g., support vector machines, decision trees, ensemble models such as a random forest)\n",
    "3. Streamlining the decision rules given here for generalization and scalability (e.g., automating the whole process from start to finish, adding a suite of models to provide more options for different datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. High Level Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level, I proceed in the following manner:\n",
    "1. Check all the files given to me by opening them and reviewing them by the naked eye\n",
    "2. Identify the desirable format of each datum provided in the files should be\n",
    "3. Create dataframes in the software tool (Python in this case) used. This includes successfully importing the files and reformating the data to the appropriate formats.\n",
    "4. Identify comprehensive set of scenarios in which data may have quality issues\n",
    "5. Develop decision logic on how to handle missingness and errors\n",
    "6. Check for missingness and errors and implement decision logic in previous step on any missing values\n",
    "7. Set key assumptions and identify KPIs for assessing model performance and conducting model selection\n",
    "8. Identify the universe of model frameworks to consider and the order in which to approach them\n",
    "9. Identify training, validation, testing, and prediction datasets. This includes determining the validation approach (if one should is tobe used) and identifying the split ratios\n",
    "10. Develop models, identify parameter and hyper parameter tuning criteria (this involves trial and error; I provide the end results only here) and assess performance of each model\n",
    "11. Select winner model(s) and generate predictions for prediction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Assumptions & Objective Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make the following assumptions:\n",
    "1. Any omissions of comments from the corpus is independent of its label value (i.e., a missing comment from the corpus provides us with no additional insight on its likelihood being labelled zero or one)\n",
    "2. Blanks are considered as missing words (however, if a word is equal to a space \" \" or several spaces, we will read it as a word)\n",
    "3. Case differences between the coded words may imply they are different words (e.g., \"Aa\" and \"aa\" may be different and hence I will not treat them as the same)\n",
    "4. The label value less commonly found in the dataset is of more interest than the common one. If both values are equally common, then I will randomly pick one\n",
    "5. The labelling of the comments we wish to predict is based on the same criteria used in labelling the labelled comments (i.e., labelled dataset has the same probability distribution as the unlabelled dataset)\n",
    "6. The order number given (second column of the \"corpus.txt\" file) represents the position of the word in a comment and thus the highest order number represents the last word of the comment. If an order number is less than one or not a number, it is mislabelled\n",
    "\n",
    "Based on the assumption and information given to me, I formalize the problem in to the following components:\n",
    "1. Data point: Comment\n",
    "2. Features of a sample data point: Words in a comment presented in the given order (top to bottom in the \"corpus.txt\" file)\n",
    "3. Sample: Collection of comments\n",
    "4. Target variable: Label taking either a value of zero or a value of one\n",
    "\n",
    "Thus, this is a binary classification problem with an array of words (of possible varying length for each comment) as an input and the label value as an output. Therefore, I need to build a model that can accurately predict labels.\n",
    "\n",
    "Here, accuracy is defined as the number of data points correctly labeled divided by total number of data points in the prediction data set. However, care should be taken when assessing accuracy because a dataset with imbalanced label value frequencies may lead to a high accuracy percentage if simply opted for a set-everything-to-common-label-value strategy. Example: if \"0\" appears 95% of the data set, then a model that is 90% accurate may not be considered as impressive if we are interested in accurately predicting the \"1\" values. Thus, given the assumptions above, we will place the f1 score as the primary KPI we wish to maximize. Mathematically, f1 is the harmonic unweighted average (e.g., please see [wiki definition](https://en.wikipedia.org/wiki/Harmonic_mean)) of precision (precision = # true positives / (# true positives + # false positives), where positive reflects the rarer label value) and recall (recall = # true positives / (# true positives + # false negatives), where positive reflects the rarer label value).\n",
    "\n",
    "In the journey of finding the best model to predict, I set the following criteria:\n",
    "1. Begin with default or standard parameter and hyper parameter assumption settings given by the software packages used (unless the model is being built from scratch). If performance (as defined in the data set criteria below) is within acceptable tolerance levels, proceed to the next model framework\n",
    "2. Begin with the simplest model framework and proceed to higher complexity\n",
    "3. Testing data set:  \n",
    "..a. Choose only models that have accuracy > set-everything-to-common-label-value strategy accuracy  \n",
    "..b. Choose a simple model over a more complex one if |f1_complex - f1_simple| <= 2% I am assuming that smaller differences are due to sample volatility. This does not become an issue as the sample size becomes very large (e.g., ten fold the current size) \n",
    "4. Training data set:   \n",
    "..a. accuracy > set-everything-to-common-label-value strategy accuracy  \n",
    "..b. f1 > 60%  \n",
    "5. Validation data set (if there is one):  \n",
    "..a. accuracy > set-everything-to-common-label-value strategy accuracy  \n",
    "..b. Choose the hyper parameters that maximize f1  \n",
    "..c. No need to recalibrate hyper parameters if f1 > 90%  \n",
    "..d. Time to run iterations <= 10 minutes or # iterations >= 100 --> Relax this requirement if no model emerges meeting earlier requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data Management Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Creating Dataframes for \"corpus.txt\" File & \"id_label.csv\" Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries needed\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"C://Users//bksin//Desktop//NLP//corpus.txt\" #Where I stored the files in my computer\n",
    "labels_path = \"C://Users//bksin//Desktop//NLP//id_label.csv\" #Where I stored the files in my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>order</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>voujm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>kvspoh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>qpjou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>dsbaz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment  order    word\n",
       "0        0      1      hp\n",
       "1        0      2   voujm\n",
       "2        0      3  kvspoh\n",
       "3        0      4   qpjou\n",
       "4        0      5   dsbaz"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading corpus file\n",
    "corpus_df = pd.read_csv(corpus_path, sep=\",\", header=None, encoding=\"latin1\")\n",
    "#A \"UnicodeDecodeError\" error is raised if I try opening the file using this line of code:\n",
    "#pd.read_csv(corpus_path, sep=\",\", header=None), which uses the standard \"utf-8\" format. Using \"latin1\" words\n",
    "#as it is able to read in special characters\n",
    "#Conducted a sanity check to make sure words with non-ascii characters such as the one in comment=5010, order=6, \n",
    "#e.g., checking corpus_df[corpus_df[\"comment\"] == 5010, were preserved)\n",
    "\n",
    "corpus_df.columns = [\"comment\", \"order\", \"word\"]#Label naming\n",
    "\n",
    "#Taking a look at a sample of the corpus file\n",
    "corpus_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment  label\n",
       "0        0    0.0\n",
       "1        1    0.0\n",
       "2        2    1.0\n",
       "3        3    0.0\n",
       "4        4    0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading labels file\n",
    "labels_df = pd.read_csv(labels_path, header=None)#No problems found here\n",
    "\n",
    "labels_df.columns = [\"comment\", \"label\"]\n",
    "\n",
    "labels_df_labelled = labels_df[labels_df[\"label\"].notnull()] #Labelled sample\n",
    "labels_df_unlabelled = labels_df[labels_df[\"label\"].isnull()] #Unlabelled (i.e., prediction) sample\n",
    "\n",
    "#Taking a look at a sample of the labels file\n",
    "labels_df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Handling Erroneous & Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.1 List of Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list provides all the scenarios I accounted for when parsing the corpus, labelled label, and unlabelled label dataframes:\n",
    "1. If corpus dataframe has blank comments\n",
    "2. If corpus dataframe has blank words (i.e., word == \"\")\n",
    "3. If corpus dataframe has order values that are not numerical, blank, or not in the correct range (i.e., <1)\n",
    "4. If corpus dataframe has comments with orders that are sequences that do not increment by one (e.g., {1, 2, 5, 6})\n",
    "5. If corpus dataframe has duplicated orders for the same comment\n",
    "6. If labelled label dataframe has comments with a label value that is a number that is not zero or one\n",
    "7. If labelled label or unlabelled label dataframe has missing comments: Remove rows with blank comments \n",
    "8. If labelled label or unlabelled label dataframe has duplicated comments\n",
    "9. If format of comment in corpus is different from format of comment labelled label and unlabelled label dataframes\n",
    "10. If label dataframe has any additional comments over corpus dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2 Implementation of Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.2.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def remove_blanks(df, column_name, data_type):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Filter out row blank values of a given column\n",
    "        \n",
    "        Arguments:\n",
    "        df -- Pandas dataframe\n",
    "        column_name -- Column name of interest we wish to filter out the blanks\n",
    "        desired_data_type -- Data type of column\n",
    "        \n",
    "        Return:\n",
    "        df_filtered -- Filtered dataframe with given column represented numerically if possible\n",
    "    \"\"\"\n",
    "    df_filtered = deepcopy(df)\n",
    "    try:\n",
    "        if data_type == \"str\":\n",
    "            df_filtered = df_filtered[df_filtered[column_name] != \"\"] #Treat the column as a string\n",
    "        else:\n",
    "             df_filtered = df[df_filtered[column_name].notnull()] #Treat the column as a numeric\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return df_filtered\n",
    "\n",
    "#Unit tests\n",
    "d_str1 = {\"A\": pd.Series([\"1\", \"2\", \"3\"]), \"B\": pd.Series([4, 5, 6])}\n",
    "d_str1_df = pd.DataFrame(data=d_str1)\n",
    "\n",
    "d_str2 = {\"A\": pd.Series([\"\", \"2\", \"3\"]), \"B\": pd.Series([4, 5, 6])}\n",
    "d_str2_df = pd.DataFrame(data=d_str2)\n",
    "\n",
    "d_num1 = {\"A\": pd.Series([1, 2, 3]), \"B\": pd.Series([4, 5, 6])}\n",
    "d_num1_df = pd.DataFrame(data=d_num1)\n",
    "\n",
    "d_num2 = {\"A\": pd.Series([np.nan, 2, 3]), \"B\": pd.Series([4, 5, 6])}\n",
    "d_num2_df = pd.DataFrame(data=d_num2)\n",
    "\n",
    "\n",
    "column_name = \"A\"\n",
    "dt_str1 = \"str\"\n",
    "dt_str2 = \"str\"\n",
    "dt_num1 = \"int\"\n",
    "dt_num2 = \"float\"\n",
    "\n",
    "unit_test1 = remove_blanks(d_str1_df, column_name, dt_str1)[column_name].tolist() == [\"1\", \"2\", \"3\"]\n",
    "unit_test2 = remove_blanks(d_str2_df, column_name, dt_str2)[column_name].tolist() == [\"2\", \"3\"]\n",
    "unit_test3 = remove_blanks(d_num1_df, column_name, dt_num1)[column_name].tolist() == [1, 2, 3]\n",
    "unit_test4 = remove_blanks(d_num2_df, column_name, dt_num2)[column_name].tolist() == [2.0, 3.0]\n",
    "unit_test = unit_test1 and unit_test2 and unit_test3 and unit_test4\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def find_duplicates(item_list):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Identify duplicates from a particular list\n",
    "        \n",
    "        Arguments:\n",
    "        item_list -- List of items we wish to find duplicates in (can be any type of list)\n",
    "        \n",
    "        Return:\n",
    "        duplicates_list -- List of duplicated values\n",
    "    \"\"\"\n",
    "    df_duplicates = []\n",
    "    try:\n",
    "        for item in item_list:\n",
    "            if item_list.count(item) > 1 and item not in df_duplicates:\n",
    "                df_duplicates.append(item)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return df_duplicates\n",
    "\n",
    "#Unit tests\n",
    "d_list = [1, 1, 2, 2, 2, 3]\n",
    "unit_test = (find_duplicates(d_list) == [1, 2])\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(df, column_names, keep):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Deduplicate redundant rows for given columns\n",
    "        \n",
    "        Arguments:\n",
    "        df -- Pandas dataframe\n",
    "        column_name -- Column names of interest we wish to deduplicate\n",
    "        \n",
    "        Return:\n",
    "        df_deduplicated -- Deduplicated dataframe with given columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_deduplicated = deepcopy(df)\n",
    "        df_deduplicated = df_deduplicated.drop_duplicates(column_names, keep=keep)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return df_deduplicated\n",
    "\n",
    "#Unit tests\n",
    "d = {\"A\": pd.Series([1, 1, 2]), \"B\": pd.Series([4, 5, 6])}\n",
    "d_df = pd.DataFrame(data=d)\n",
    "\n",
    "column_names = [\"A\"]\n",
    "keep1 = \"first\"\n",
    "keep2 = False\n",
    "\n",
    "unit_test1 = remove_duplicates(d_df, column_names, keep1)[\"B\"].tolist() == [4, 6]\n",
    "unit_test2 = remove_duplicates(d_df, column_names, keep2)[\"B\"].tolist() == [6]\n",
    "unit_test = unit_test1 and unit_test2\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def consistent_list(item_list):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Check if all items in a list are equal\n",
    "        \n",
    "        Arguments:\n",
    "        item_list -- List of items (can be of any type)\n",
    "        \n",
    "        Return:\n",
    "        same_items -- Boolean variable indicating whether or not all items in the list are equal to each other\n",
    "    \"\"\"\n",
    "    same_items = True\n",
    "    try:\n",
    "        item0 = item_list[0]\n",
    "        for item in item_list:\n",
    "            if item0 != item:\n",
    "                same_items = False\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    return same_items\n",
    "\n",
    "#Unit tests\n",
    "item_list_str1 = [\"a\", \"a\", \"a\"]\n",
    "item_list_str2 = [\"a\", \"b\", \"a\"]\n",
    "item_list_num1 = [1, 1, 1]\n",
    "item_list_num2 = [1, 2, 1]\n",
    "\n",
    "unit_test1 = (consistent_list(item_list_str1) == True)\n",
    "unit_test2 = (consistent_list(item_list_str2) == False)\n",
    "unit_test3 = (consistent_list(item_list_num1) == True)\n",
    "unit_test4 = (consistent_list(item_list_num2) == False)\n",
    "unit_test = unit_test1 and unit_test2 and unit_test3 and unit_test4\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.2.2 Corpus Dataframe Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. If corpus dataframe has blank comments: Remove rows with blank comments \n",
    "column_name = \"comment\"\n",
    "data_type = corpus_df.comment.dtype\n",
    "corpus_df = remove_blanks(corpus_df, column_name, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. If corpus dataframe has blank words (i.e., word == \"\"): Leave as is. They will be used in modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. If corpus dataframe has order values that are not numerical, blank, or not in the correct range (i.e., <1):\n",
    "#   Coerce orders to be numerical Remove rows with blank orders or not in the correct range\n",
    "column_name = \"order\"\n",
    "data_type = corpus_df.order.dtype\n",
    "\n",
    "temp = deepcopy(corpus_df) #This deepcopy is used to avoid a chained indexing warning in the next step\n",
    "temp[\"order\"] = pd.to_numeric(temp[\"order\"], errors=\"coerce\")\n",
    "temp = remove_blanks(temp, column_name, data_type)#Removing any NA's captured created by the pd.to_numeric function\n",
    "\n",
    "temp[\"order\"] = temp[\"order\"].astype(int) #Converting the numeric type into integers\n",
    "\n",
    "temp = temp[temp[\"order\"] >= 1]\n",
    "temp = temp.sort_values(by=[\"comment\", \"order\"])#Now that order is an integer, we can sort it\n",
    "corpus_df = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. If corpus dataframe has comments with orders that are sequences that do not increment by one (e.g., {1, 2, 5, 6})\n",
    "#Addressing missing in-between order values will be a criterion to consider in modelling assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. If corpus dataframe has duplicated orders for the same comment:\n",
    "##a. If words are the same, deduplicate and keep 1\n",
    "##b. If there are different words, deduplicate and keep 1 and replace word with a blank\n",
    "\n",
    "#Addressing condition a. by deduplicated repeated words\n",
    "list_of_comments_corpus = set(corpus_df[\"comment\"].unique())\n",
    "\n",
    "\n",
    "column_names = [\"comment\", \"order\", \"word\"]\n",
    "keep = \"first\"\n",
    "corpus_df = remove_duplicates(corpus_df, column_names, keep)#Deduplicate at word level\n",
    "\n",
    "#Addressing condition b.\n",
    "for comment in list_of_comments_corpus:\n",
    "    list_of_orders = corpus_df[corpus_df[\"comment\"] == comment][\"order\"].tolist()\n",
    "    duplicate_orders = find_duplicates(list_of_orders)\n",
    "\n",
    "    for order in duplicate_orders:\n",
    "        column_names = [\"comment\", \"order\"]\n",
    "        keep = \"first\"\n",
    "        corpus_df = remove_duplicates(corpus_df, column_names, keep)#Deduplicate at order level\n",
    "        \n",
    "        #Now each order has one word and we wish to replace this word with a blank\n",
    "        #First we need to find row number of this word\n",
    "        ind = corpus_df.index[(corpus_df[\"comment\"] == comment) & (corpus_df[\"order\"] == order)].tolist()[0]\n",
    "        #Now we can set the word to blank\n",
    "        corpus_df.loc[ind, \"word\"] = \"\"\n",
    "\n",
    "corpus_df = corpus_df.replace(np.nan, \"\", regex=True) #Replacing all possibly existing NaNs with blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.2.2 Labelled and unlabelled Label Dataframes Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment  label\n",
       "0        0      0\n",
       "1        1      0\n",
       "2        2      1\n",
       "3        3      0\n",
       "4        4      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. If labelled label dataframe has comments with a label value that is a number that is not zero or one: \n",
    "#   Remove comment from labelled dataset\n",
    "#First, we convert the labels into numeric values if they are not numeric\n",
    "\n",
    "temp = deepcopy(labels_df_labelled) #This deepcopy is used to avoid a chained indexing warning in the next step\n",
    "temp[\"label\"] = pd.to_numeric(temp[\"label\"], errors=\"coerce\")\n",
    "\n",
    "column_name = \"label\"\n",
    "data_type = temp.label.dtype\n",
    "\n",
    "temp = remove_blanks(temp, column_name, data_type) #Removing any NA's captured created by the pd.to_numeric function\n",
    "temp[\"label\"] = temp[\"label\"].astype(int) #Converting the numeric type into integers\n",
    "\n",
    "labels_df_labelled = temp[(temp[\"label\"] == 0) | (temp[\"label\"] == 1)]\n",
    "labels_df_labelled[0:5] #Check that labels are in integer form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. If labelled label or unlabelled label dataframe has missing comments: Remove rows with blank comments \n",
    "column_name = \"comment\"\n",
    "data_type = labels_df_labelled.comment.dtype\n",
    "labels_df_labelled = remove_blanks(labels_df_labelled, column_name, data_type)\n",
    "\n",
    "data_type = labels_df_unlabelled.comment.dtype\n",
    "labels_df_unlabelled = remove_blanks(labels_df_unlabelled, column_name, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. If labelled label or unlabelled label dataframe has duplicated comments:\n",
    "##a. If labelled and label values are the same, deduplicate and keep 1\n",
    "##b. If labelled and there are at least two comments with different label values, remove comment\n",
    "##c. If unlabelled, deduplicate and keep 1\n",
    "column_names = [\"comment\"]\n",
    "\n",
    "##Addressing labelled label dataframe:\n",
    "list_of_comments_labels_labelled = set(labels_df_labelled[\"comment\"].unique())\n",
    "\n",
    "for comment in list_of_comments_labels_labelled:\n",
    "    list_of_labels = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()\n",
    "    same_labels = consistent_list(list_of_labels) #Check if all labels are the same\n",
    "\n",
    "    if same_labels: #Deduplicate comments\n",
    "        keep = \"first\"\n",
    "    else: #Remove all comments\n",
    "        keep = False\n",
    "    \n",
    "    labels_df_labelled[labels_df_labelled[\"comment\"] == comment] = remove_duplicates(labels_df_labelled[\\\n",
    "                                                                   labels_df_labelled[\"comment\"] == comment],\\\n",
    "                                                                                     column_names, keep)\n",
    "\n",
    "#since the above line is a slice, the outcome will be creating NaNs, so they can be remove in the next few lines\n",
    "column_name = \"comment\"\n",
    "data_type = labels_df_labelled.comment.dtype\n",
    "labels_df_labelled = remove_blanks(labels_df_labelled, column_name, data_type)\n",
    "\n",
    "##Addressing unlabelled label dataframe:\n",
    "keep = \"first\"\n",
    "labels_df_unlabelled = remove_duplicates(labels_df_unlabelled, column_names, keep)#Deduplicate at comment level\n",
    "\n",
    "#Note: If a comment is found both in the labelled and unlabelled label dataframes, then we do not need\n",
    "#to predict this comment's outcome. I simply use the value found in the labelled label dataframe\n",
    "#This will be incorporated in the modelling module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.2.3 Addressing Potential Incompatilibility Issues between Corpus Dataframe and Labelled and Unlabelled Label dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "#9. If format of comment in corpus is different from format of comment labelled label and unlabelled label dataframes\n",
    "#   Force all formats to be strings\n",
    "\n",
    "corpus_comment_dtype = corpus_df.comment.dtype\n",
    "labels_labelled_dtype = labels_df_labelled.comment.dtype\n",
    "labels_unlabelled_dtype = labels_df_labelled.comment.dtype\n",
    "\n",
    "if (corpus_comment_dtype != labels_labelled_dtype) or (corpus_comment_dtype != labels_unlabelled_dtype):\n",
    "    corpus_df[\"comment\"] = corpus_df[\"comment\"].astype(str)\n",
    "    labels_df_labelled[\"comment\"] = labels_df_labelled[\"comment\"].astype(str)\n",
    "    labels_df_unlabelled[\"comment\"] = labels_df_unlabelled[\"comment\"].astype(str)\n",
    "    \n",
    "#Unit tests:\n",
    "unit_test1 = (corpus_df.comment.dtype == labels_df_labelled.comment.dtype)\n",
    "unit_test2 = (corpus_df.comment.dtype == labels_df_unlabelled.comment.dtype)\n",
    "unit_test = unit_test1 and unit_test2\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3374}\n",
      "{4822}\n"
     ]
    }
   ],
   "source": [
    "#10. If label dataframe has any additional comments over corpus dataframe:  \n",
    "#..a. If missing comment is in labelled dataset, remove comment from labelled dataset  \n",
    "#..b. If missing comment is in unlabelled dataset, label value will be estimated using a random number \n",
    "#generator of a Bernoulli random variable with probability of success equal to frequency of the less\n",
    "#common label divided by sample size of labelled dataset that will be used for model development\n",
    "\n",
    "list_of_comments_corpus = set(corpus_df[\"comment\"].unique())\n",
    "list_of_comments_labels_labelled = set(labels_df_labelled[\"comment\"].unique())\n",
    "list_of_comments_labels_unlabelled = set(labels_df_unlabelled[\"comment\"].unique())\n",
    "\n",
    "label_extra_comments_labelled = list_of_comments_labels_labelled - list_of_comments_corpus\n",
    "label_extra_comments_unlabelled = list_of_comments_labels_unlabelled - list_of_comments_corpus\n",
    "\n",
    "print(label_extra_comments_labelled)##You will see comment \"3374\" is missing\n",
    "print(label_extra_comments_unlabelled)##You will see comment \"4822\" is missing\n",
    "\n",
    "#These two sets will be used when developing the model making the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Modelling Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bksin\\desktop\\jbeast_trader\\env\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#RNN packages\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import keras.backend as K\n",
    "\n",
    "#Elastic net packages\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#Layered NN packages\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "#Calculation and plotting packages\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "#Timing package\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Preparing Modelling & Prediction Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have four samples:\n",
    "#(1) Modelling sample --> All comments that can be trained, validated, and tested\n",
    "#(2) Prediction sample via deterministic --> All comments whose labels will be calculated by knowing the\n",
    "#    value. This occurs in the case that the same comment is found in the training dataset\n",
    "#(3) Prediction sample via model --> All comments whose labels will be predicted via a model\n",
    "#(4) Prediction sample via random --> All comments whose labels will be predicted via random number generator\n",
    "\n",
    "comments_for_modelling = list_of_comments_labels_labelled - label_extra_comments_labelled\n",
    "comments_for_prediction_deterministic = []\n",
    "comments_for_prediction_model = []\n",
    "comments_for_prediction_random = []\n",
    "\n",
    "for comment in list_of_comments_labels_unlabelled:\n",
    "    if comment in comments_for_modelling:\n",
    "        comments_for_prediction_deterministic.append(comment)\n",
    "    elif comment in label_extra_comments_unlabelled:\n",
    "        comments_for_prediction_random.append(comment)\n",
    "    else:\n",
    "        comments_for_prediction_model.append(comment)\n",
    "\n",
    "comments_for_prediction_deterministic = set(comments_for_prediction_deterministic) \n",
    "comments_for_prediction_model = set(comments_for_prediction_model)\n",
    "comment_for_prediction_random = set(comments_for_prediction_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Conducting Preliminary Analytics on Modelling Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADvhJREFUeJzt3X+Q3HV9x/HniwSKFlQ051iTQCgGp5n+kJlrtGNnpCOUwNRER8oEi9aWkv5R6ihUh/6QUtTpVNrB0dIftCoWK5iiaEpjoxaYtig2RwHHwMTG+CMRLAcCVUAh5d0/du8z2+Vyd5B8by/J8zFzM/vd/ezuezPknvv9frNLqgpJkgAOG/UAkqSFwyhIkhqjIElqjIIkqTEKkqTGKEiSGqOgA16Sm5P8xnzfVzoYGQUtGEm+keSUUc8xJcklSZ5I8v2Bn3eMei6pS4tHPYC0wH28qs6ZaUGSAKmqJ+dpJqkz7ilowUtyTJIbkkwmebB/ednQshOS/EeSh5N8OsnzB+7/iiRfSPJQkjuTnLwfZro5yXuS3AI8Cvx4kucm+WCSe5N8O8m7kyzqr1+U5E+T3J9kZ5LfSlJJFvdv/397Sf29lI/O5TX0Z3lXkluSfC/JZ5MsGbj95wfuuyvJm5P8bJL/nnr+/rrXJ7ljX/9sdGAzCjoQHAZ8GDgOOBZ4DPjzoTVvAn4deDGwB3g/QJKlwD8B7waeD/wO8IkkY/thrjcCG4CjgW8CH+k/90uAk4BfBKbOV5wH/FL/+nHgzLk+yRxfwxuAXwNeCBzRX0OSY4HPAB8AxoCXAXdU1VbgAeDUgcc4B7h6rnPp4GQUtOBV1QNV9YmqerSqvge8B3jV0LKrq+orVfUI8E7grP679HOAzVW1uaqerKrPARPAGXN8+rP677Cnfl48cNtVVbWtqvbQ+2V9OvDWqnqkqu4DLgfWTz0O8L6q2lVV3wX++Gn8EczlNXy4qr5aVY8BG+n98gf4FeDzVXVNVT3R/7Oc2hv4SP+x6e9ZnQZ87GnMpYOQ5xS04CV5Nr1fsGuAY/pXH51kUVX9b39718BdvgkcDiyht3fxy0leM3D74cBNc3z6jTOcUxh8zuP6j3tv7xQD0HvTNbXmxdPMOFdzeQ3fGbj8KHBU//Jy4Gt7edyPAncnOYpetP6tqu59GnPpIGQUdCC4EHgp8PKq+k6SlwG3AxlYs3zg8rHAE8D99H4RX11V53Uw1+BXDO8Cfggs6e85DLt3mhkHPQI8e2D7RUOP/Uxfwy5g9XQ3VNW3k3wReB29Q2F/+QweXwcZDx9poTk8yZEDP4vpHbN/DHiof5jjD6e53zlJVvX3Ki4FruvvRXwUeE2S0/one49McvI0J6r3Sf8d9meBP0vynCSHJTkhydRhro3AW5IsS3IMcNHQQ9wBrE9yeJLhcw778hr+HjglyVlJFid5QT+qU/4OeAfwU8D1T/+V62BjFLTQbKYXgKmfS4D3Ac+i987/VuCfp7nf1cBV9A6jHAm8BaCqdgHrgN8DJum9c3473fy3/yZ6J3nvAh4ErgN+rH/b3wBbgDuB/wQ+OXTfdwIn9O/3Rwwc29+X11BV36J37uFC4Lv04vMzA0uup3d46vr++Rgd4uL/ZEeaf0lWAF8HDt/L4ab5nOVrwG9W1edHOYcWBvcUpENYktfTOzdy46hn0cLgiWbpEJXkZmAV8EY/ja0pHj6SJDUePpIkNQfc4aMlS5bUihUrRj2GJB1QbrvttvuratavdzngorBixQomJiZGPYYkHVCSzOlT9B4+kiQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNQfcJ5r3xeWf++qoRzhgve3UE0c9gqR54J6CJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJKaTqOQZE2S7Ul2JLlomtuPTXJTktuTfDnJGV3OI0maWWdRSLIIuAI4HVgFnJ1k1dCyPwA2VtVJwHrgL7qaR5I0uy73FFYDO6pqZ1U9DlwLrBtaU8Bz+pefC9zT4TySpFl0+dXZS4FdA9u7gZcPrbkE+GyS3wZ+FDilw3kkSbPock8h01xXQ9tnA1dV1TLgDODqJE+ZKcmGJBNJJiYnJzsYVZIE3UZhN7B8YHsZTz08dC6wEaCqvggcCSwZfqCqurKqxqtqfGxsrKNxJUldRmErsDLJ8UmOoHciedPQmm8BrwZI8hP0ouCugCSNSGdRqKo9wPnAFuBuev/KaFuSS5Os7S+7EDgvyZ3ANcCbq2r4EJMkaZ50+v9orqrNwOah6y4euHwX8MouZ5AkzZ2faJYkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlS02kUkqxJsj3JjiQX7WXNWUnuSrItyce6nEeSNLPFXT1wkkXAFcCpwG5ga5JNVXXXwJqVwO8Cr6yqB5O8sKt5JEmz63JPYTWwo6p2VtXjwLXAuqE15wFXVNWDAFV1X4fzSJJm0WUUlgK7BrZ3968bdCJwYpJbktyaZM10D5RkQ5KJJBOTk5MdjStJ6jIKmea6GtpeDKwETgbOBv42yfOecqeqK6tqvKrGx8bG9vugkqSeLqOwG1g+sL0MuGeaNZ+uqieq6uvAdnqRkCSNQJdR2AqsTHJ8kiOA9cCmoTWfAn4BIMkSeoeTdnY4kyRpBp1Foar2AOcDW4C7gY1VtS3JpUnW9pdtAR5IchdwE/D2qnqgq5kkSTPr7J+kAlTVZmDz0HUXD1wu4IL+jyRpxPxEsySpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJKapxWFJK9IcmOSW5K8tquhJEmjsXimG5O8qKq+M3DVBcBaIMAXgE91OJskaZ7NGAXgr5LcBlxWVT8AHgLeADwJ/E/Xw0mS5teMh4+q6rXAHcANSd4IvJVeEJ4NePhIkg4ys55TqKp/BE4Dngd8EtheVe+vqsmuh5Mkza8Zo5BkbZJ/B24EvgKsB16X5JokJ8zHgJKk+TPbOYV3Az8HPAvYXFWrgQuSrATeQy8SkqSDxGxReJjeL/5nAfdNXVlV/4VBkKSDzmznFF5H76TyHnr/6kiSdBCbcU+hqu4HPjBPs0iSRsyvuZAkNUZBktR0GoUka5JsT7IjyUUzrDszSSUZ73IeSdLMOotCkkXAFcDpwCrg7CSrpll3NPAW4EtdzSJJmpsu9xRWAzuqamdVPQ5cC6ybZt27gPcCP+hwFknSHHQZhaXAroHt3f3rmiQnAcur6oaZHijJhiQTSSYmJ/12DUnqSpdRyDTXVbsxOQy4HLhwtgeqqiuraryqxsfGxvbjiJKkQV1GYTewfGB7GXDPwPbRwE8CNyf5BvAKYJMnmyVpdLqMwlZgZZLjkxxB72sxNk3dWFUPV9WSqlpRVSuAW4G1VTXR4UySpBl0FoWq2gOcD2wB7gY2VtW2JJcmWdvV80qSnrnZvhBvn1TVZmDz0HUX72XtyV3OIkmanZ9oliQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVLTaRSSrEmyPcmOJBdNc/sFSe5K8uUk/5LkuC7nkSTNrLMoJFkEXAGcDqwCzk6yamjZ7cB4Vf00cB3w3q7mkSTNrss9hdXAjqraWVWPA9cC6wYXVNVNVfVof/NWYFmH80iSZtFlFJYCuwa2d/ev25tzgc9Md0OSDUkmkkxMTk7uxxElSYO6jEKmua6mXZicA4wDl013e1VdWVXjVTU+Nja2H0eUJA1a3OFj7waWD2wvA+4ZXpTkFOD3gVdV1Q87nEeSNIsu9xS2AiuTHJ/kCGA9sGlwQZKTgL8G1lbVfR3OIkmag86iUFV7gPOBLcDdwMaq2pbk0iRr+8suA44C/iHJHUk27eXhJEnzoMvDR1TVZmDz0HUXD1w+pcvnlyQ9PX6iWZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVLT6ddcSNKwyz/31VGPcMB626kndv4c7ilIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqSm0ygkWZNke5IdSS6a5vYfSfLx/u1fSrKiy3kkSTPrLApJFgFXAKcDq4Czk6waWnYu8GBVvQS4HPiTruaRJM2uyz2F1cCOqtpZVY8D1wLrhtasAz7Sv3wd8Ook6XAmSdIMFnf42EuBXQPbu4GX721NVe1J8jDwAuD+wUVJNgAb+pvfT7K9k4lHbwlDr32huGDUA0jzY8H+HYR9/nt43FwWdRmF6d7x1zNYQ1VdCVy5P4ZayJJMVNX4qOeQDlX+Hez28NFuYPnA9jLgnr2tSbIYeC7w3Q5nkiTNoMsobAVWJjk+yRHAemDT0JpNwK/2L58J3FhVT9lTkCTNj84OH/XPEZwPbAEWAR+qqm1JLgUmqmoT8EHg6iQ76O0hrO9qngPEQX+ITFrgDvm/g/GNuSRpip9oliQ1RkGS1BiFBWC2rwOR1K0kH0pyX5KvjHqWUTMKIzbHrwOR1K2rgDWjHmIhMAqjN5evA5HUoar6V/yMFGAUFoLpvg5k6YhmkXSIMwqjN6ev+pCk+WAURm8uXwciSfPCKIzeXL4ORJLmhVEYsaraA0x9HcjdwMaq2jbaqaRDS5JrgC8CL02yO8m5o55pVPyaC0lS456CJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSmv8DwkGlXBkypC8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size of modelling sample is  3999\n",
      "Target label we will use is  1\n",
      "Number of unique words  8713\n"
     ]
    }
   ],
   "source": [
    "list_of_words_master = sorted(corpus_df.word.unique().tolist()) #List of all words in corpus\n",
    "m = len(comments_for_modelling)\n",
    "n = len(list_of_words_master)\n",
    "\n",
    "m_1 = 0\n",
    "comment_length_list_1_10 = 0\n",
    "comment_length_list_11_20 = 0\n",
    "comment_length_list_21_30 = 0\n",
    "comment_length_list_31_40 = 0\n",
    "comment_length_list_41_50 = 0\n",
    "comment_length_list_51_max = 0\n",
    "\n",
    "for comment in comments_for_modelling:#Counting all of the ones given in comment_for_modelling\n",
    "    m_1 += labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    comment_length = len(corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist())\n",
    "    if comment_length <= 10:\n",
    "        comment_length_list_1_10 += 1\n",
    "    elif comment_length <= 20:\n",
    "        comment_length_list_11_20 += 1\n",
    "    elif comment_length <= 30:\n",
    "        comment_length_list_21_30 += 1\n",
    "    elif comment_length <= 40:\n",
    "        comment_length_list_31_40 += 1\n",
    "    elif comment_length <= 50:\n",
    "        comment_length_list_41_50 += 1\n",
    "    else:\n",
    "        comment_length_list_51_max += 1\n",
    "m_0 = m - m_1\n",
    "\n",
    "x_pos = [0, 1]\n",
    "labels = (\"0\", \"1\")\n",
    "label_freq = [m_0/m * 1.0, m_1/m * 1.0]\n",
    " \n",
    "plt.bar(x_pos, label_freq, 0.35, align='center', alpha=0.5)\n",
    "plt.xticks(x_pos, labels)\n",
    "plt.ylabel(\"%\")\n",
    "plt.title(\"Label Frequency\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "target_label_master = (m_1 < m_0) * 1\n",
    "\n",
    "print(\"Sample size of modelling sample is \", m)\n",
    "print(\"Target label we will use is \", target_label_master)\n",
    "print(\"Number of unique words \", n)\n",
    "\n",
    "#As the graph shows below, the label 1 is much less than label zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Looking at comment length distribution. This can give guidance on setting up model variables and explain certain\n",
    "#results (e.g., adding padding may yield poor results as the majority of comments are ten or less)\n",
    "x_pos = [0, 1, 2, 3, 4, 5]\n",
    "labels = (\"[1-10]\", \"[11-20]\", \"[21-30]\", \"[31-40]\", \"[40-50]\", \"50+\")\n",
    "label_freq = [comment_length_list_1_10, comment_length_list_11_20, comment_length_list_21_30,\\\n",
    "              comment_length_list_31_40,comment_length_list_41_50, comment_length_list_51_max]\n",
    " \n",
    "plt.bar(x_pos, label_freq, 0.35, align='center', alpha=0.5)\n",
    "plt.xticks(x_pos, labels)\n",
    "plt.ylabel(\"Comment length bins\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comment Length Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Splitting the Labelled Label Dataset into Training, Validation, & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given that the sample size is quite large, 3999, I will begin our analysis with a simple standard split of\n",
    "#80%(train), 10%(validate), and 10%(test) . Should I consistently see significant differences between the \n",
    "#validation and testing data sets, I may revisit the sampling method (e.g., use k-cross validation, increasing\n",
    "#number of iterations and combinations of hyper-parameters considered). For now, I am optiming for the simpler \n",
    "#option and then will iterate if necessary.\n",
    "#For cases where a validation dataset is not needed, the training and validation samples are combined. Thus, all\n",
    "#models evaluated on the same testing dataset\n",
    "\n",
    "#Obtaining the entire vocabulary of list of words\n",
    "np.random.seed(0)#Setting a random seed\n",
    "split_train = 0.8\n",
    "split_validate = 0.1\n",
    "\n",
    "comments_for_modelling_training = []\n",
    "comments_for_modelling_validation = []\n",
    "comments_for_modelling_testing = []\n",
    "\n",
    "for comment in comments_for_modelling:\n",
    "    split_rand = np.random.rand()\n",
    "    if split_rand <= split_train:\n",
    "        comments_for_modelling_training.append(comment)\n",
    "    elif split_rand <= split_train + split_validate:\n",
    "        comments_for_modelling_validation.append(comment)\n",
    "    else:\n",
    "        comments_for_modelling_testing.append(comment)\n",
    "\n",
    "comments_for_modelling_training = set(comments_for_modelling_training)\n",
    "comments_for_modelling_validation = set(comments_for_modelling_validation)\n",
    "comments_for_modelling_testing = set(comments_for_modelling_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Building & Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.1 Model Exploration Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model frameworks that could be used for this problem are Naive Bayes, Linear (or logistic) regression with elastic net regularization, support vector machines (SVMs), decision trees, ensemble models (e.g., random forests), standard multi-layered fully connect neural networks (L-layered NN), and recurrent neural networks (RNNs). The strategy is to begin with simple model frameworks and proceed to complex model frameworks without necessarily covering every model type unless models used so far are not performing well.\n",
    "\n",
    "I divide the frameworks I explore into two types:\n",
    "1. Models that use feature representation represented by word count --> Order of words is not important, just the frequency\n",
    "2. Models that index the words --> Words are read in sequence\n",
    "\n",
    "I begin by tackling the first type, which is based on simpler assumptions. First, I use the Naive Bayes model framework, and then a linear regression with elastic net regularization. I will explore models in the second type (L-layered NNs and RNNs). If none of the model frameworks meet predictive specifications, then I will explore other types of model frameworks and potentially iterate on more hyperparameters.\n",
    "\n",
    "Note: In the first iteration of model exploration, from simple to complex, missing order values that may be present in the corpus dataframe are ignored, in the sense that all words are taken to be as consecutive. Example: {1: \"Thor\", 3: \"Fantastic\"} is read as [\"Thor\", \"Fantastic\"] without introducing a filler missing value (e.g., [\"Thor\", \"*\", \"Fantastic\"]). This assumption can be relaxed (assuming that there are missing order values in the corpus dataframe, which is actually the case in this project, but may not be in future datasets) in subsequent iterations of this project, if none of the model frameworks meet predictive specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.2 Global Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def get_feature_matrix(X, Y, list_of_words):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Provide a feature matrix that represents word counts for each comment and the label for each comment\n",
    "    \n",
    "        Arguments:\n",
    "        X -- List of comments of shape length=sample size, where each comment is a list of words\n",
    "        Y -- List of labels, where each label value is either equal to zero or one, of length=sample size\n",
    "        list_of_words -- List of all words available in the corpus\n",
    "    \n",
    "        Returns:\n",
    "        feature_matrix -- Numpy array of shape = (sample size, number of words + 1)\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    n = len(list_of_words)\n",
    "    feature_matrix = np.zeros((m, n+1))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(n+1):\n",
    "            if j < n:\n",
    "                feature_matrix[i, j] = X[i].count(list_of_words[j])\n",
    "            else:\n",
    "                feature_matrix[i, j] = Y[i]\n",
    "   \n",
    "    return feature_matrix\n",
    "\n",
    "#Unit tests:\n",
    "X = [[\"Thor\", \"is\", \"awesome\"], [\"Sky\", \"is\", \"green\"]]\n",
    "Y = [1, 0]\n",
    "list_of_words = [\"awesome\", \"is\", \"green\", \"Sky\", \"Thor\"]\n",
    "\n",
    "fm_hat = get_feature_matrix(X, Y, list_of_words)\n",
    "fm_true = np.array([[1, 1, 0, 0, 1, 1], [0, 1, 1, 1, 0, 0]])\n",
    "\n",
    "unit_test = True\n",
    " \n",
    "for i in range(2):\n",
    "    for j in range(6):\n",
    "        unit_testi = (fm_true[i,j] == fm_hat[i,j])\n",
    "        unit_test = unit_test and unit_testi\n",
    "\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def assess_model(Y, Y_hat, target_label, print_performance):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Provide kpi's of model\n",
    "    \n",
    "        Arguments:\n",
    "        Y -- List of true labels, where each label value is either equal to zero or one, of length=sample size\n",
    "        Y_hat -- List of predicted labels, where each label value is either equal to zero or one, of length=sample size\n",
    "        target_label -- Binary integer, either zero or one, indicating which Y variable is the target label\n",
    "        print_performance -- Boolean variable that is used to determine whether or not to print kpis\n",
    "    \n",
    "        Returns:\n",
    "        accuracy -- A percentage that is equal to (true positive+true negative) / sample size\n",
    "        precision -- A percentage that is equal to true positive / (true positive + false positive)\n",
    "        recall -- A percentage that is equal to true positive / (true positive + false negative)\n",
    "        f1 -- A percentage that is the f1 score and is equal to 2/ (1/precision + 1/recall).\n",
    "              In other words, it is the harmonic average of precision and recall\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    m = len(Y)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    for i in range(m):\n",
    "        if Y[i] == target_label and Y_hat[i] == target_label:#True positive casse\n",
    "            tp += 1\n",
    "        elif Y[i] != target_label and Y_hat[i] == target_label:#False positive case\n",
    "            fp += 1\n",
    "        elif Y[i] == target_label and Y_hat[i] != target_label:#False negative case\n",
    "            fn += 1\n",
    "        elif Y[i] != target_label and Y_hat[i] != target_label:#True negative case\n",
    "            tn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / m\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    f1 = 2 / (1/ (precision + epsilon) + 1/(recall + epsilon))\n",
    "    \n",
    "    if print_performance:\n",
    "        print(\"# true positives = \", tp)\n",
    "        print(\"# true negatives = \", tn)\n",
    "        print(\"# false positives = \", fp)\n",
    "        print(\"# false negatives = \", fn)\n",
    "        \n",
    "        print(\"Baseline strategy {:.2%}\".format((tn+fp)/m))#All negatives/sample size\n",
    "        print(\"accuracy = {:.2%}\".format(accuracy))\n",
    "        print(\"precision = {:.2%}\".format(precision))\n",
    "        print(\"recall = {:.2%}\".format(recall))\n",
    "        print(\"f1 = {:.2%}\".format(f1))\n",
    "        \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "#Unit tests:\n",
    "Y = [1, 1, 0, 0, 0, 0]\n",
    "Y_hat = [1, 0, 1, 1, 0, 0]\n",
    "target_label = 1\n",
    "print_performance = False\n",
    "accuracy, precision, recall, f1 = assess_model(Y, Y_hat, target_label, print_performance)\n",
    "\n",
    "epsilon = 1e-10\n",
    "unit_test1 = (accuracy == (1 + 2) / 6)\n",
    "unit_test2 = (precision == 1 / (1 + 2 + epsilon))\n",
    "unit_test3 = (recall == 1 / (1 + 1 + epsilon))\n",
    "unit_test4 = (f1 == 2 / (1/(precision + epsilon) + 1/(recall + epsilon))) \n",
    "unit_test = unit_test1 and unit_test2 and unit_test3 and unit_test4\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.3 Model Framework 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.3.1 Assumptions & Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model framework is based on the following two key assumptions:\n",
    "1. Word order is not important\n",
    "2. Words given label are conditionally independent of each other\n",
    "\n",
    "Inputs: I use word count as the feature input vector for each comment\n",
    "Output: Probability of each label value occurring. The label value chosen is the the one that has the higher probability of occurring\n",
    "\n",
    "Note: A validation sample is not required for this model as there are no hyper parameters to calibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.3.2 Local Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def get_prob_word_given_label(feature_matrix, target_label):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Calculate the probabilities of any word occurring given a label value\n",
    "    \n",
    "        Arguments:\n",
    "        feature_matrix -- Numpy array of shape = (sample size, number of words + 1)\n",
    "        target_label -- Binary integer, either zero or one, indicating which Y variable is the target label\n",
    "    \n",
    "        Returns:\n",
    "        probabilities -- Tuple of values: (prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg)\n",
    "                         prob_word_given_pos_vector --> Numpy array of shape (number of words, 1)\n",
    "                         prob_word_given_neg_vector --> Numpy array of shape (number of words, 1)\n",
    "                         prob_pos --> Scalar value between zero and one, representing probability of getting target \n",
    "                                      label\n",
    "                         prob_neg --> Scalar value between zero and one, representing probability of getting \n",
    "                                      non-target label\n",
    "    \"\"\"\n",
    "    m, n = feature_matrix.shape\n",
    "    n = n - 1\n",
    "    \n",
    "    prob_word_given_pos_vector = np.zeros((n, 1))\n",
    "    prob_word_given_neg_vector = np.zeros((n, 1))\n",
    "    epsilon = 1e-10 #Used for avoiding a division by zero scenario\n",
    "    \n",
    "    for j in range(n):\n",
    "        count_pos = 0\n",
    "        count_neg = 0\n",
    "        for i in range(m):\n",
    "            if feature_matrix[i, n] == target_label:\n",
    "                count_pos += feature_matrix[i, j]\n",
    "            else:\n",
    "                count_neg += feature_matrix[i, j]\n",
    "\n",
    "        prob_word_given_pos_vector[j] = count_pos\n",
    "        prob_word_given_neg_vector[j] = count_neg\n",
    "\n",
    "    prob_word_given_pos_vector /= (np.sum(prob_word_given_pos_vector) + epsilon)\n",
    "    prob_word_given_neg_vector /= (np.sum(prob_word_given_neg_vector) + epsilon)\n",
    "\n",
    "    prob_pos = np.sum(feature_matrix[:, n]) / m\n",
    "    prob_neg = 1 - prob_pos\n",
    "    \n",
    "    probabilities = (prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg)\n",
    "    return probabilities\n",
    "\n",
    "#Unit tests:\n",
    "epsilon = 1e-10\n",
    "feature_matrix = np.array([[1,2,1],[1,2,0],[0,1,0]])\n",
    "target_label = 1\n",
    "\n",
    "probabilities = get_prob_word_given_label(feature_matrix, target_label)\n",
    "(prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg) = probabilities\n",
    "\n",
    "unit_test1 = (prob_word_given_pos_vector[0] == 1 / (3 + epsilon))[0]\n",
    "unit_test2 = (prob_word_given_pos_vector[1] == 2 / (3 + epsilon))[0]\n",
    "unit_test3 = (prob_word_given_neg_vector[0] == 1 / (4 + epsilon))[0]\n",
    "unit_test4 = (prob_word_given_neg_vector[1] == 3 / (4 + epsilon))[0]\n",
    "unit_test3 = (prob_word_given_neg_vector[0] == 1 / (4 + epsilon))[0]\n",
    "unit_test5 = (prob_pos == 1./3)\n",
    "unit_test6 = (prob_neg == 1 - prob_pos)\n",
    "unit_test = unit_test1 and unit_test2 and unit_test3 and unit_test4 and unit_test5 and unit_test6\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test pass =  True\n"
     ]
    }
   ],
   "source": [
    "def predict_Y_naive_bayes(X, list_of_words, probabilities):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Estimate labels via the Naive Bayes model framework\n",
    "\n",
    "        Arguments:\n",
    "        X -- List of comments of shape length=sample size, where each comment is a list of words\n",
    "        list_of_words -- List of all words available in the corpus\n",
    "        probabilities -- Tuple of values: (prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg)\n",
    "                         prob_word_given_pos_vector --> Numpy array of shape (number of words, 1)\n",
    "                         prob_word_given_neg_vector --> Numpy array of shape (number of words, 1)\n",
    "                         prob_pos --> Scalar value between zero and one, representing probability of getting target \n",
    "                         label\n",
    "                         prob_neg --> Scalar value between zero and one, representing probability of getting \n",
    "                                      non-target label\n",
    "\n",
    "        Returns:\n",
    "        Y_hat -- List of predicted labels, where each label value is either equal to zero or one, of length=sample size\n",
    "    \"\"\"\n",
    "    \n",
    "    (prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg) = probabilities\n",
    "    m = len(X)\n",
    "    epsilon = 1e-10 #Used for avoiding a division by zero scenario\n",
    "    \n",
    "    Y_hat = []\n",
    "    for i in range(m):\n",
    "        comment = X[i]\n",
    "        yhat_pos = np.log(prob_pos+epsilon)#Using logarithms for numerical stability\n",
    "        yhat_neg = np.log(prob_neg+epsilon)#Using logarithms for numerical stability\n",
    "\n",
    "        for word in comment:       \n",
    "            j = list_of_words.index(word)\n",
    "            yhat_pos += np.log(prob_word_given_pos_vector[j]+epsilon)\n",
    "            yhat_neg += np.log(prob_word_given_neg_vector[j]+epsilon)\n",
    "        \n",
    "        y_hat_np= (yhat_pos > yhat_neg) * 1#List containing a numpy array\n",
    "        Y_hat.append(y_hat_np[0])\n",
    "    \n",
    "    return Y_hat\n",
    "\n",
    "#Unit tests:\n",
    "epsilon = 1e-10\n",
    "prob_word_given_pos_vector = np.array([[1 / (3 + epsilon)], [2/ (3 + epsilon)]])\n",
    "prob_word_given_neg_vector = np.array([[3 / (4 + epsilon)], [1/ (4 + epsilon)]])\n",
    "prob_pos = 2./3\n",
    "prob_neg = 1 - prob_pos\n",
    "\n",
    "X = [[\"Hulk\", \"good\"], [\"Hulk\"], [\"good\"]]\n",
    "list_of_words = [\"good\", \"Hulk\"]\n",
    "probabilities = (prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg)\n",
    "Y_hat = predict_Y_naive_bayes(X, list_of_words, probabilities)\n",
    "\n",
    "unit_test = (Y_hat == [1, 1, 0])\n",
    "print(\"Unit test pass = \", unit_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.3.2 Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling training and testing datasets\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for comment in comments_for_modelling_training.union(comments_for_modelling_validation):\n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    X_train.append(x)\n",
    "    Y_train.append(y)\n",
    "    \n",
    "for comment in comments_for_modelling_testing: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    X_test.append(x)\n",
    "    Y_test.append(y)\n",
    "\n",
    "list_of_words = list_of_words_master\n",
    "feature_matrix = get_feature_matrix(X_train, Y_train, list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 0.26 minutes\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "target_label = target_label_master\n",
    "\n",
    "tic = time.clock()\n",
    "probabilities = get_prob_word_given_label(feature_matrix, target_label)\n",
    "(prob_word_given_pos_vector, prob_word_given_neg_vector, prob_pos, prob_neg) = probabilities\n",
    "toc = time.clock()\n",
    "\n",
    "time_taken = (toc - tic) / 60 \n",
    "print(\"Time taken = {:.2f} minutes\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# true positives =  478\n",
      "# true negatives =  3082\n",
      "# false positives =  6\n",
      "# false negatives =  3\n",
      "Baseline strategy 86.52%\n",
      "accuracy = 99.75%\n",
      "precision = 98.76%\n",
      "recall = 99.38%\n",
      "f1 = 99.07%\n"
     ]
    }
   ],
   "source": [
    "#Assessing performance of training dataset\n",
    "list_of_words = list_of_words_master\n",
    "Y_hat_train = predict_Y_naive_bayes(X_train, list_of_words, probabilities)\n",
    "print_performance = True\n",
    "accuracy_train, precision_train, recall_train, f1_train = assess_model(Y_train, Y_hat_train, target_label,\\\n",
    "                                                                       print_performance)\n",
    "#Results below show that the training dataset is a good fit:\n",
    "#Accuracy > baseline\n",
    "#f1 is quite high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# true positives =  51\n",
      "# true negatives =  374\n",
      "# false positives =  2\n",
      "# false negatives =  3\n",
      "Baseline strategy 87.44%\n",
      "accuracy = 98.84%\n",
      "precision = 96.23%\n",
      "recall = 94.44%\n",
      "f1 = 95.33%\n"
     ]
    }
   ],
   "source": [
    "#Assessing performance of testing dataset\n",
    "list_of_words = list_of_words_master\n",
    "Y_hat_test = predict_Y_naive_bayes(X_test, list_of_words, probabilities)\n",
    "print_performance = True\n",
    "accuracy_test, precision_test, recall_test, f1_test = assess_model(Y_test, Y_hat_test, target_label,\\\n",
    "                                                                       print_performance)\n",
    "#Results below show that the testing dataset is a good fit:\n",
    "#Accuracy > baseline\n",
    "#f1 is quite high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.3.3 Predicting Unlabelled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list_predict = []\n",
    "X_predict = []\n",
    "Y_hat_predict_naive_bayes = []\n",
    "\n",
    "for comment in comments_for_prediction_model: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    X_predict.append(x)\n",
    "    comment_list_predict.append(comment)\n",
    "\n",
    "list_of_words = list_of_words_master\n",
    "Y_hat_predict_naive_bayes = predict_Y_naive_bayes(X_predict, list_of_words, probabilities)\n",
    "\n",
    "for comment in comments_for_prediction_deterministic:\n",
    "    y_hat = label_df_labelled[corpus_df[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_naive_bayes.append(y_hat)\n",
    "\n",
    "target_label = target_label_master\n",
    "if target_label == 1:\n",
    "    p = np.sum(Y_train)/len(Y_train)\n",
    "else:\n",
    "    p = 1 - np.sum(Y_train)/len(Y_train)\n",
    "\n",
    "rand_seed_counter = 0\n",
    "for comment in comments_for_prediction_random:\n",
    "    np.random.seed(rand_seed_counter)\n",
    "    if np.random.rand() < p:\n",
    "        y_hat = target_label\n",
    "    else:\n",
    "        y_hat = 1 - target_label\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_naive_bayes.append(y_hat)\n",
    "    rand_seed_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments  labels\n",
       "0      4000       0\n",
       "1      4001       0\n",
       "2      4002       0\n",
       "3      4003       0\n",
       "4      4004       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_candidate_naive_bayes = {\"comments\": comment_list_predict, \"labels\": Y_hat_predict_naive_bayes}\n",
    "predict_candidate_naive_bayes_df = pd.DataFrame(data=predict_candidate_naive_bayes)\n",
    "predict_candidate_naive_bayes_df = predict_candidate_naive_bayes_df.sort_values(by=[\"comments\"])\n",
    "predict_candidate_naive_bayes_df  = predict_candidate_naive_bayes_df.reset_index(drop=True)\n",
    "predict_candidate_naive_bayes_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = \"C://Users//bksin//Desktop//NLP//labels_predict_naive_bayes.csv\"#Where I stored the files in my computer\n",
    "predict_candidate_naive_bayes_df.to_csv(predict_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.3.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes approach is an excellent fit suggesting that the presence of certain words in a comment is the dominating factor for predicting label value. Furthermore, it poses a great advantage over more complex model frameworks in its similicity and speed, which can be crucial if computational power is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.4 Model Framework 2: Linear Regression with Elastic Net Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.4.1 Assumptions & Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model framework is based on the following two key assumptions:\n",
    "1. Word order is not important\n",
    "2. Words given label are conditionally independent of each other\n",
    "\n",
    "Inputs: I use word count as the feature input vector for each comment  \n",
    "Output: Probability of each label value occurring. The label value chosen is the the one that has the higher probability of occurring\n",
    "\n",
    "Note: A validation dataset will be used to find the optimal combination of the following two hyper parameters (I was not able to achieve an f1 > 90% with default parameter values and hence will aim to optimize hyper parameters):\n",
    "1. alpha --> sum of the L1 and L2 regularization parameters\n",
    "2. l1_ratio --> L1 regularization term / alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.4.2 Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling training, validation, and testing datasets\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_validate = []\n",
    "Y_validate = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for comment in comments_for_modelling_training:\n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    X_train.append(x)\n",
    "    Y_train.append(y)\n",
    "\n",
    "for comment in comments_for_modelling_validation: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    X_validate.append(x)\n",
    "    Y_validate.append(y)\n",
    "    \n",
    "for comment in comments_for_modelling_testing: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    X_test.append(x)\n",
    "    Y_test.append(y)\n",
    "\n",
    "list_of_words = list_of_words_master\n",
    "feature_matrix_train = get_feature_matrix(X_train, Y_train, list_of_words)\n",
    "feature_matrix_validate = get_feature_matrix(X_validate, Y_validate, list_of_words)\n",
    "feature_matrix_test = get_feature_matrix(X_test, Y_test, list_of_words)\n",
    "\n",
    "#Formatting the input and outputs to be used by the sklearn function ElasticNet()\n",
    "X_train_e = feature_matrix_train[:,:-1]\n",
    "Y_train_e = feature_matrix_train[:,-1]\n",
    "X_validate_e = feature_matrix_validate[:,:-1]\n",
    "Y_validate_e = feature_matrix_validate[:,-1]\n",
    "X_test_e = feature_matrix_test[:,:-1]\n",
    "Y_test_e = feature_matrix_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 , accuracy_train = 99.06% , accuracy_validate = 98.20% , f1_validate = 92.13% , Time taken = 0.07 minutes\n",
      "Model 2 , accuracy_train = 97.77% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.03 minutes\n",
      "Model 3 , accuracy_train = 98.52% , accuracy_validate = 97.42% , f1_validate = 88.64% , Time taken = 0.05 minutes\n",
      "Model 4 , accuracy_train = 99.81% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.32 minutes\n",
      "Model 5 , accuracy_train = 96.64% , accuracy_validate = 96.13% , f1_validate = 81.48% , Time taken = 0.03 minutes\n",
      "Model 6 , accuracy_train = 97.83% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.03 minutes\n",
      "Model 7 , accuracy_train = 99.50% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.09 minutes\n",
      "Model 8 , accuracy_train = 86.36% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 9 , accuracy_train = 99.56% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.17 minutes\n",
      "Model 10 , accuracy_train = 99.72% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.27 minutes\n",
      "Model 11 , accuracy_train = 99.40% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.08 minutes\n",
      "Model 12 , accuracy_train = 98.62% , accuracy_validate = 97.68% , f1_validate = 89.89% , Time taken = 0.06 minutes\n",
      "Model 13 , accuracy_train = 97.64% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.04 minutes\n",
      "Model 14 , accuracy_train = 99.84% , accuracy_validate = 98.20% , f1_validate = 91.95% , Time taken = 0.21 minutes\n",
      "Model 15 , accuracy_train = 95.98% , accuracy_validate = 95.10% , f1_validate = 75.95% , Time taken = 0.02 minutes\n",
      "Model 16 , accuracy_train = 99.21% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.07 minutes\n",
      "Model 17 , accuracy_train = 96.07% , accuracy_validate = 96.65% , f1_validate = 84.71% , Time taken = 0.03 minutes\n",
      "Model 18 , accuracy_train = 98.49% , accuracy_validate = 97.42% , f1_validate = 88.64% , Time taken = 0.05 minutes\n",
      "Model 19 , accuracy_train = 98.52% , accuracy_validate = 97.42% , f1_validate = 88.64% , Time taken = 0.07 minutes\n",
      "Model 20 , accuracy_train = 98.77% , accuracy_validate = 97.68% , f1_validate = 89.89% , Time taken = 0.05 minutes\n",
      "Model 21 , accuracy_train = 97.30% , accuracy_validate = 96.39% , f1_validate = 83.33% , Time taken = 0.03 minutes\n",
      "Model 22 , accuracy_train = 86.83% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 23 , accuracy_train = 98.11% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.04 minutes\n",
      "Model 24 , accuracy_train = 87.21% , accuracy_validate = 88.40% , f1_validate = 8.16% , Time taken = 0.02 minutes\n",
      "Model 25 , accuracy_train = 96.16% , accuracy_validate = 95.88% , f1_validate = 80.00% , Time taken = 0.02 minutes\n",
      "Model 26 , accuracy_train = 97.30% , accuracy_validate = 96.39% , f1_validate = 83.33% , Time taken = 0.04 minutes\n",
      "Model 27 , accuracy_train = 86.45% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 28 , accuracy_train = 90.85% , accuracy_validate = 90.21% , f1_validate = 32.14% , Time taken = 0.02 minutes\n",
      "Model 29 , accuracy_train = 93.90% , accuracy_validate = 93.04% , f1_validate = 59.70% , Time taken = 0.02 minutes\n",
      "Model 30 , accuracy_train = 89.81% , accuracy_validate = 90.21% , f1_validate = 32.14% , Time taken = 0.02 minutes\n",
      "Model 31 , accuracy_train = 88.31% , accuracy_validate = 88.92% , f1_validate = 15.69% , Time taken = 0.02 minutes\n",
      "Model 32 , accuracy_train = 86.70% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 33 , accuracy_train = 96.57% , accuracy_validate = 95.88% , f1_validate = 80.00% , Time taken = 0.03 minutes\n",
      "Model 34 , accuracy_train = 88.05% , accuracy_validate = 88.92% , f1_validate = 15.69% , Time taken = 0.02 minutes\n",
      "Model 35 , accuracy_train = 88.24% , accuracy_validate = 88.92% , f1_validate = 15.69% , Time taken = 0.02 minutes\n",
      "Model 36 , accuracy_train = 97.45% , accuracy_validate = 96.39% , f1_validate = 83.33% , Time taken = 0.04 minutes\n",
      "Model 37 , accuracy_train = 98.40% , accuracy_validate = 97.42% , f1_validate = 88.64% , Time taken = 0.05 minutes\n",
      "Model 38 , accuracy_train = 86.36% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 39 , accuracy_train = 86.77% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 40 , accuracy_train = 86.95% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 41 , accuracy_train = 94.31% , accuracy_validate = 94.07% , f1_validate = 69.33% , Time taken = 0.02 minutes\n",
      "Model 42 , accuracy_train = 98.96% , accuracy_validate = 98.20% , f1_validate = 92.13% , Time taken = 0.06 minutes\n",
      "Model 43 , accuracy_train = 94.40% , accuracy_validate = 93.30% , f1_validate = 61.76% , Time taken = 0.02 minutes\n",
      "Model 44 , accuracy_train = 87.61% , accuracy_validate = 88.40% , f1_validate = 8.16% , Time taken = 0.02 minutes\n",
      "Model 45 , accuracy_train = 99.81% , accuracy_validate = 98.20% , f1_validate = 91.95% , Time taken = 0.14 minutes\n",
      "Model 46 , accuracy_train = 97.83% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.04 minutes\n",
      "Model 47 , accuracy_train = 98.81% , accuracy_validate = 97.68% , f1_validate = 89.89% , Time taken = 0.05 minutes\n",
      "Model 48 , accuracy_train = 91.57% , accuracy_validate = 90.98% , f1_validate = 40.68% , Time taken = 0.02 minutes\n",
      "Model 49 , accuracy_train = 86.48% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.03 minutes\n",
      "Model 50 , accuracy_train = 86.36% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 51 , accuracy_train = 95.10% , accuracy_validate = 94.07% , f1_validate = 67.61% , Time taken = 0.03 minutes\n",
      "Model 52 , accuracy_train = 99.81% , accuracy_validate = 98.20% , f1_validate = 91.95% , Time taken = 0.14 minutes\n",
      "Model 53 , accuracy_train = 98.11% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.04 minutes\n",
      "Model 54 , accuracy_train = 97.36% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.03 minutes\n",
      "Model 55 , accuracy_train = 99.84% , accuracy_validate = 98.20% , f1_validate = 91.95% , Time taken = 0.20 minutes\n",
      "Model 56 , accuracy_train = 99.62% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.16 minutes\n",
      "Model 57 , accuracy_train = 95.63% , accuracy_validate = 94.85% , f1_validate = 73.68% , Time taken = 0.03 minutes\n",
      "Model 58 , accuracy_train = 96.95% , accuracy_validate = 96.39% , f1_validate = 82.93% , Time taken = 0.03 minutes\n",
      "Model 59 , accuracy_train = 98.21% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.05 minutes\n",
      "Model 60 , accuracy_train = 98.99% , accuracy_validate = 98.20% , f1_validate = 92.13% , Time taken = 0.07 minutes\n",
      "Model 61 , accuracy_train = 97.77% , accuracy_validate = 96.65% , f1_validate = 84.71% , Time taken = 0.04 minutes\n",
      "Model 62 , accuracy_train = 98.52% , accuracy_validate = 97.42% , f1_validate = 88.64% , Time taken = 0.04 minutes\n",
      "Model 63 , accuracy_train = 96.54% , accuracy_validate = 96.65% , f1_validate = 84.71% , Time taken = 0.03 minutes\n",
      "Model 64 , accuracy_train = 95.63% , accuracy_validate = 94.85% , f1_validate = 73.68% , Time taken = 0.02 minutes\n",
      "Model 65 , accuracy_train = 95.32% , accuracy_validate = 94.59% , f1_validate = 72.00% , Time taken = 0.02 minutes\n",
      "Model 66 , accuracy_train = 95.19% , accuracy_validate = 94.85% , f1_validate = 74.36% , Time taken = 0.02 minutes\n",
      "Model 67 , accuracy_train = 95.35% , accuracy_validate = 94.59% , f1_validate = 72.00% , Time taken = 0.02 minutes\n",
      "Model 68 , accuracy_train = 98.18% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.04 minutes\n",
      "Model 69 , accuracy_train = 98.77% , accuracy_validate = 97.68% , f1_validate = 89.89% , Time taken = 0.05 minutes\n",
      "Model 70 , accuracy_train = 97.30% , accuracy_validate = 96.65% , f1_validate = 84.34% , Time taken = 0.03 minutes\n",
      "Model 71 , accuracy_train = 96.10% , accuracy_validate = 95.10% , f1_validate = 75.32% , Time taken = 0.02 minutes\n",
      "Model 72 , accuracy_train = 99.78% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.24 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 73 , accuracy_train = 98.02% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.05 minutes\n",
      "Model 74 , accuracy_train = 99.81% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.25 minutes\n",
      "Model 75 , accuracy_train = 99.81% , accuracy_validate = 98.20% , f1_validate = 91.95% , Time taken = 0.13 minutes\n",
      "Model 76 , accuracy_train = 99.34% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.08 minutes\n",
      "Model 77 , accuracy_train = 97.27% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.04 minutes\n",
      "Model 78 , accuracy_train = 99.65% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.19 minutes\n",
      "Model 79 , accuracy_train = 97.96% , accuracy_validate = 96.91% , f1_validate = 86.05% , Time taken = 0.03 minutes\n",
      "Model 80 , accuracy_train = 86.99% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 81 , accuracy_train = 97.33% , accuracy_validate = 96.39% , f1_validate = 83.33% , Time taken = 0.03 minutes\n",
      "Model 82 , accuracy_train = 99.72% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.26 minutes\n",
      "Model 83 , accuracy_train = 98.68% , accuracy_validate = 97.68% , f1_validate = 89.89% , Time taken = 0.07 minutes\n",
      "Model 84 , accuracy_train = 96.35% , accuracy_validate = 95.88% , f1_validate = 80.00% , Time taken = 0.03 minutes\n",
      "Model 85 , accuracy_train = 90.82% , accuracy_validate = 90.21% , f1_validate = 32.14% , Time taken = 0.02 minutes\n",
      "Model 86 , accuracy_train = 86.36% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 87 , accuracy_train = 86.45% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 88 , accuracy_train = 91.86% , accuracy_validate = 91.49% , f1_validate = 45.90% , Time taken = 0.02 minutes\n",
      "Model 89 , accuracy_train = 99.81% , accuracy_validate = 98.45% , f1_validate = 93.18% , Time taken = 0.26 minutes\n",
      "Model 90 , accuracy_train = 86.36% , accuracy_validate = 87.89% , f1_validate = 0.00% , Time taken = 0.02 minutes\n",
      "Model 91 , accuracy_train = 98.49% , accuracy_validate = 97.42% , f1_validate = 88.64% , Time taken = 0.04 minutes\n",
      "Model 92 , accuracy_train = 94.25% , accuracy_validate = 93.30% , f1_validate = 61.76% , Time taken = 0.02 minutes\n",
      "Model 93 , accuracy_train = 98.40% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.04 minutes\n",
      "Model 94 , accuracy_train = 98.27% , accuracy_validate = 97.16% , f1_validate = 87.36% , Time taken = 0.04 minutes\n",
      "Model 95 , accuracy_train = 95.00% , accuracy_validate = 94.07% , f1_validate = 68.49% , Time taken = 0.02 minutes\n",
      "Model 96 , accuracy_train = 88.40% , accuracy_validate = 89.18% , f1_validate = 19.23% , Time taken = 0.02 minutes\n",
      "Model 97 , accuracy_train = 99.84% , accuracy_validate = 98.20% , f1_validate = 91.95% , Time taken = 0.30 minutes\n",
      "Model 98 , accuracy_train = 96.60% , accuracy_validate = 96.39% , f1_validate = 83.33% , Time taken = 0.03 minutes\n",
      "Model 99 , accuracy_train = 87.87% , accuracy_validate = 89.18% , f1_validate = 19.23% , Time taken = 0.02 minutes\n",
      "Model 100 , accuracy_train = 96.35% , accuracy_validate = 96.13% , f1_validate = 81.93% , Time taken = 0.02 minutes\n",
      "Time taken = 6.21 minutes\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "#Hyper parameter bounds were set by trial and error\n",
    "target_label = target_label_master\n",
    "\n",
    "tic_all = time.clock()\n",
    "print_performance = False\n",
    "iterations = 100 #Each iteration takes about 1/10 minutes on my laptop (2.6 GH, 16GB RAM)\n",
    "\n",
    "model_list = []\n",
    "f1_validate_best = 0\n",
    "\n",
    "#Calculating base_line_strategy_accuracy\n",
    "if target_label == 1:\n",
    "    baseline_strategy_accuracy_train = 1 - np.sum(Y_train_e)/Y_train_e.shape[0]\n",
    "    baseline_strategy_accuracy_validate = 1 - np.sum(Y_validate_e)/Y_validate_e.shape[0]\n",
    "    baseline_strategy_accuracy_test = 1 - np.sum(Y_test_e)/Y_test_e.shape[0]\n",
    "\n",
    "else:\n",
    "    baseline_strategy_accuracy_train = np.sum(Y_train_e)/Y_train_e.shape[0]\n",
    "    baseline_strategy_accuracy_validate = np.sum(Y_validate_e)/Y_validate_e.shape[0]\n",
    "    baseline_strategy_accuracy_test = np.sum(Y_test_e)/Y_test_e.shape[0]\n",
    "\n",
    "for i in range(iterations):\n",
    "    tic_one = time.clock()\n",
    "    \n",
    "    alpha = 10**(-3*np.random.rand())#Hyper parameter randomization\n",
    "    l1_ratio = 10**(-3*np.random.rand() - 1)#Hyper parameter randomization\n",
    "    \n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=0)\n",
    "    \n",
    "    model.fit(X_train_e, Y_train_e)\n",
    "    \n",
    "    model_list.append(model)\n",
    "        \n",
    "    Y_hat_train_e = model.predict(X_train_e)\n",
    "    Y_hat_train_e = (Y_hat_train_e > 0.5) * 1\n",
    "    \n",
    "    accuracy_train, precision_train, recall_train, f1_train = assess_model(Y_train_e, Y_hat_train_e, target_label,\\\n",
    "                                                                           print_performance)\n",
    "    Y_hat_validate_e = model.predict(X_validate_e)\n",
    "    Y_hat_validate_e = (Y_hat_validate_e > 0.5) * 1\n",
    "    accuracy_validate, precision_validate, recall_validate, f1_validate = assess_model(Y_validate_e, \\\n",
    "                                                                                       Y_hat_validate_e, target_label,\\\n",
    "                                                                                       print_performance)\n",
    "    if i == 0:\n",
    "        f1_validate_best = f1_validate\n",
    "        best_model = model\n",
    "    else:\n",
    "        if accuracy_train > baseline_strategy_accuracy_train and\\\n",
    "        accuracy_validate> baseline_strategy_accuracy_validate and\\\n",
    "        f1_validate > f1_validate_best:\n",
    "            f1_validate_best = f1_validate\n",
    "            best_model = model\n",
    "    \n",
    "    toc_one = time.clock()       \n",
    "    time_taken_one = (toc_one - tic_one) / 60\n",
    "    print(\"Model {}\".format(i+1), \", accuracy_train = {:.2%}\".format(accuracy_train).format(l1_ratio),\\\n",
    "          \", accuracy_validate = {:.2%}\".format(accuracy_validate), \", f1_validate = {:.2%}\".format(f1_validate),\\\n",
    "          \", Time taken = {:.2f} minutes\".format(time_taken_one))\n",
    "    \n",
    "toc_all = time.clock()\n",
    "time_taken_all = (toc_all - tic_all) / 60 \n",
    "print(\"Time taken = {:.2f} minutes\".format(time_taken_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# true positives =  49\n",
      "# true negatives =  376\n",
      "# false positives =  0\n",
      "# false negatives =  5\n",
      "Baseline strategy 87.44%\n",
      "accuracy = 98.84%\n",
      "precision = 100.00%\n",
      "recall = 90.74%\n",
      "f1 = 95.15%\n"
     ]
    }
   ],
   "source": [
    "#Assessing performance of testing dataset\n",
    "target_label = target_label_master\n",
    "Y_hat_test_e = best_model.predict(X_test_e)\n",
    "Y_hat_test_e = (Y_hat_test_e > 0.5) * 1\n",
    "\n",
    "print_performance = True\n",
    "accuracy_test, precision_test, recall_test, f1_test = assess_model(Y_test_e, Y_hat_test_e, target_label,\\\n",
    "                                                                                   print_performance)\n",
    "#Results below show that the testing dataset is a good fit:\n",
    "#Accuracy > baseline\n",
    "#f1 is quite high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.4.3 Predicting Unlabelled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = target_label_master\n",
    "comment_list_predict = []\n",
    "X_predict = []\n",
    "\n",
    "for comment in comments_for_prediction_model: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    X_predict.append(x)\n",
    "    comment_list_predict.append(comment)\n",
    "\n",
    "list_of_words = list_of_words_master\n",
    "\n",
    "Y_predict = [0]*len(X_predict)#Just used to run the feature matrix\n",
    "feature_matrix_predict = get_feature_matrix(X_predict, Y_predict, list_of_words)\n",
    "X_predict_e = feature_matrix_predict[:,:-1]\n",
    "\n",
    "Y_hat_predict_e = best_model.predict(X_predict_e)\n",
    "Y_hat_predict_e = (Y_hat_predict_e > 0.5) * 1\n",
    "Y_hat_predict_e = Y_hat_predict_e.tolist()\n",
    "\n",
    "for comment in comments_for_prediction_deterministic:\n",
    "    y_hat = label_df_labelled[corpus_df[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_e.append(y_hat)\n",
    "\n",
    "target_label = target_label_master\n",
    "if target_label == 1:\n",
    "    p = np.sum(Y_train)/len(Y_train)\n",
    "else:\n",
    "    p = 1 - np.sum(Y_train)/len(Y_train)\n",
    "\n",
    "rand_seed_counter = 0\n",
    "for comment in comments_for_prediction_random:\n",
    "    np.random.seed(rand_seed_counter)\n",
    "    if np.random.rand() < p:\n",
    "        y_hat = target_label\n",
    "    else:\n",
    "        y_hat = 1 - target_label\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_e.append(y_hat)\n",
    "    rand_seed_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments  labels\n",
       "0      4000       0\n",
       "1      4001       0\n",
       "2      4002       0\n",
       "3      4003       0\n",
       "4      4004       0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_candidate_e = {\"comments\": comment_list_predict, \"labels\": Y_hat_predict_e}\n",
    "predict_candidate_e_df = pd.DataFrame(data=predict_candidate_e)\n",
    "predict_candidate_e_df = predict_candidate_e_df.sort_values(by=[\"comments\"])\n",
    "predict_candidate_e_df  = predict_candidate_e_df.reset_index(drop=True)\n",
    "predict_candidate_e_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = \"C://Users//bksin//Desktop//NLP//labels_predict_elastic_net.csv\"#Where I stored the files in my computer\n",
    "predict_candidate_e_df.to_csv(predict_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.4.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression with elastic net regularization approach is an excellent fit suggesting that the presence of certain words in a comment is the dominating factor for predicting label value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.5 Model Framework 3: L-Layered Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.5.1 Assumptions & Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model framework is based on the following two key assumptions:\n",
    "1. Word order is important\n",
    "2. All comments with number of words shorter than the maximum comment will be padded, so that all comments are of equal length, and such padding will not have a significant impact on predictions\n",
    "3. Blank values will be given a special character\n",
    "\n",
    "Inputs: Each word is converted into an index number from 1 until the maximum number of words in the corpus list of words. Padding will have a zero assigned value. All inputs are then standardized by subtracting the training sample mean index value and training sample standard deviation (I use the training sample mean and std for standardizing all datasets)  \n",
    "Output: Estimate of Y, either zero or one\n",
    "\n",
    "Note: A validation dataset will be used to find the optimal combination of the following hyper parameters (I was not able to achieve an f1 > 90% with default parameter values and hence will aim to optimize hyper parameters):\n",
    "1. Layers dimension -- Number of layers and the number of units in each layer\n",
    "2. Activation function -- Either a Rectified Linear Unit (ReLU) or tanh function for hidden layers. Output layer uses a sigmoid function since the outcome is binary\n",
    "3. L2 beta -- Hyperparameter that is used to control the effect of L2 regularization\n",
    "4. keep prob -- Hyperparameter that controls the amount of drop out (a regularization tool)  \n",
    "5. Starter learning_ ate -- Hyperparameter positive number used to control the starting point of the learning rate\n",
    "6. ADAM beta1 -- Hyperparameter non-negative number controlling exponential decay component of the ADAM algorithm\n",
    "7. ADAM_beta2 -- Hyperparameter non-negative number controlling RMSprop component of the ADAM algorithm\n",
    "8. Decay_rate -- Hyperparameter positive number used to control the rate of decay of the learning rate\n",
    "9. Number of epochs -- Number of epochs of the optimization loop\n",
    "10. Minibatch_size -- Sample size of each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.5.2 Local Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following helper functions are all used to build the L-layer NN. The majority of the code below is based on\n",
    "#assignments completed in the Coursera first and second Deep Learning specialization courses. Modifications were\n",
    "#made here to increase flexibility of tweaking hyper parameters\n",
    "\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Creates placeholders for the tensorflow session\n",
    "\n",
    "        Arguments:\n",
    "        n_x = integer representing the number of features of X\n",
    "        n_y = integer representing the number of outputs we want (in this case, n_y = 2)\n",
    "\n",
    "        Returns:\n",
    "        X -- Placeholder for the input features, of shape (n_x, None) and dtype \"float32\"\n",
    "        Y -- Placeholder for the output variables, of shape (n_y, None) and dtype \"float32\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None))\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def initialize_parameters(layers_dims):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Initialize parameters of the neural network\n",
    "\n",
    "        Arguments:\n",
    "        layers_dims -- A list of integers that provides the number of units of each layer including the first layer\n",
    "\n",
    "        Returns:\n",
    "        parameters -- A dictionary of tensors containing weight and bias variables of all layers\n",
    "        weights -- A tensor containing weights\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        nl_prev = layers_dims[l-1]\n",
    "        nl = layers_dims[l]\n",
    "        W = tf.get_variable(\"W\" + str(l), [nl, nl_prev], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(\"b\" + str(l), [nl, 1], initializer = tf.zeros_initializer())\n",
    "        \n",
    "        W_reshape = tf.reshape(W, [1, nl*nl_prev])\n",
    "        \n",
    "        if l == 1:\n",
    "            weights = W_reshape\n",
    "        else:\n",
    "            weights = tf.concat([weights, W_reshape], 1)\n",
    "        \n",
    "        parameters[\"W\" + str(l)] = W\n",
    "        parameters[\"b\" + str(l)] = b\n",
    "    \n",
    "    return parameters, weights\n",
    "\n",
    "def forward_propagation(X, parameters, activation_fn, keep_prob):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Implements the forward propagation component of the model\n",
    "\n",
    "        Arguments:\n",
    "        X -- Input data placeholder of shape (number of features, sample size)\n",
    "        parameters -- A dictionary of tensors containing weight and bias variables of all layers\n",
    "        activation_fn -- A hyperparameter that is string used to determine the activation function used in the \n",
    "                         hidden layers Currently takes on \"relu\" or \"tanh\"\n",
    "        keep_prob -- Hyperparameter that controls the amount of drop out (a regularization tool)\n",
    "\n",
    "        Returns:\n",
    "        ZL -- Placeholder of shape (2, number of units in previous layer).\n",
    "              Represents the linear unit value of the last layer\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 #The number of layers\n",
    "    A = X\n",
    "    for l in range(1, L+1):\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        \n",
    "        Z = tf.add(tf.matmul(W, A), b)\n",
    "        \n",
    "        if l < L:\n",
    "            if activation_fn == \"relu\":\n",
    "                A = tf.nn.relu(Z) #A = max(0,Z)\n",
    "            else:#Default\n",
    "                A = tf.nn.tanh(Z)    \n",
    "            drop_out = tf.nn.dropout(A, keep_prob)  #Apply drop out\n",
    "    ZL = Z\n",
    "    return ZL\n",
    "\n",
    "def compute_cost(ZL, Y, weights, l2_beta):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Computes the cost\n",
    "\n",
    "        Arguments:\n",
    "        ZL -- Placeholder of shape (1, number of units in previous layer).\n",
    "              Represents the linear unit value of the last layer\n",
    "        Y -- Placeholder for the output variables, of shape (n_y, sample_size) and dtype \"float32\"\n",
    "        weights -- Numpy array of weights of shape (1, total number of weights)\n",
    "        l2_beta -- Hyperparameter that takes on non-negative values used to control the effect of L2 regularization\n",
    "\n",
    "        Returns:\n",
    "        cost -- Tensor of the cost function\n",
    "    \"\"\"\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(Y)\n",
    "    w = tf.transpose(weights)\n",
    "    \n",
    "    regularizer = tf.nn.l2_loss(w)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels) + \\\n",
    "                          l2_beta*regularizer)\n",
    "        \n",
    "    return cost\n",
    "\n",
    "def random_minibatches(X, Y, minibatch_size):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Create a list of mini-batches from (X, Y)\n",
    "\n",
    "        Arguments:\n",
    "        X -- Input data of shape (number of features, sample size)\n",
    "        Y -- Output data of shape (number of outputs, sample size)\n",
    "        minibatch_size -- Sample size of each batch\n",
    "\n",
    "        Returns:\n",
    "        minibatches -- List of mini-batches (X_minibatch, Y_minibatch)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    minibatches = []\n",
    "    \n",
    "    #1) Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    \n",
    "    X_shuffled = X[:, permutation].reshape((X.shape[0], m)) #Handling the case that X has only one feature\n",
    "    Y_shuffled = Y[:, permutation].reshape((Y.shape[0], m)) #Handling the case that Y is only one output\n",
    "    \n",
    "    #2) Partition (shuffled_X, shuffled_Y), excluding last batch in the case that m/minibatch_size is not an integer\n",
    "    num_complete_minibatches = math.floor(m/minibatch_size)\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        X_minibatch = X_shuffled[:, k*minibatch_size: (k+1)*minibatch_size]\n",
    "        Y_minibatch = Y_shuffled[:, k*minibatch_size: (k+1)*minibatch_size]\n",
    "        \n",
    "        minibatch = (X_minibatch, Y_minibatch)\n",
    "        minibatches.append(minibatch)\n",
    "    \n",
    "    #Handling the case that m/minibatch_size is not an integer\n",
    "    if m % minibatch_size != 0:\n",
    "        X_minibatch = X_shuffled[:, num_complete_minibatches*minibatch_size: m]\n",
    "        Y_minibatch = Y_shuffled[:, num_complete_minibatches*minibatch_size: m]\n",
    "        \n",
    "        minibatch = (X_minibatch, Y_minibatch)\n",
    "        minibatches.append(minibatch)\n",
    "    \n",
    "    return minibatches\n",
    "\n",
    "\n",
    "def train_layered_nn(X_train, Y_train, layers_dims,  activation_fn, l2_beta, keep_prob, starter_learning_rate,\\\n",
    "                     adam_beta1, adam_beta2, decay_rate, num_epochs, minibatch_size, print_cost_flag):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Train the neural network\n",
    "\n",
    "        Arguments:\n",
    "        X_train -- Training input data (normalized) of shape (number of features, training sample size)\n",
    "        Y_train -- Training output data of shape (2, training sample size)\n",
    "        layers_dims -- A list of integers that provides the number of units of each layer including the first layer\n",
    "        activation_fn -- A hyperparameter that is string used to determine the activation function used in the \n",
    "                         hidden layers Currently takes on \"relu\" or \"tanh\"\n",
    "        l2_beta -- Hyperparameter that takes on non-negative values used to control the effect of L2 regularization\n",
    "        keep_prob -- Hyperparameter that controls the amount of drop out (a regularization tool)  \n",
    "        starter_learning_rate -- Hyperparameter positive number used to control the starting point of the learning \n",
    "                                 rate\n",
    "        adam_beta1 -- Hyperparameter non-negative number controlling exponential decay component of the ADAM algorithm\n",
    "        adam_beta2 -- Hyperparameter non-negative number controlling RMSprop component of the ADAM algorithm\n",
    "        decay_rate -- Hyperparameter positive number used to control the rate of decay of the learning rate\n",
    "        num_epochs -- Number of epochs of the optimization loop\n",
    "        minibatch_size -- Sample size of each batch\n",
    "        print_cost_flag -- Boolean variable that is used to determine if cost function should be printed and graphed\n",
    "\n",
    "        Returns:\n",
    "        parameters -- A dictionary containing final calibrated weight and bias variables of all layers\n",
    "    \"\"\"\n",
    "    #########################################################################################################\n",
    "    #Modelling component#\n",
    "    #########################################################################################################\n",
    "    ops.reset_default_graph() #Used to make it possible to rerun model without overwriting tf variables\n",
    "    (n_x, m) = X_train.shape\n",
    "    n_y = Y_train.shape[0]\n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    parameters, weights = initialize_parameters(layers_dims)\n",
    "    \n",
    "    kp = tf.placeholder(tf.float32, name = \"kp\")\n",
    "    \n",
    "    ZL = forward_propagation(X, parameters, activation_fn, kp)\n",
    "    cost = compute_cost(ZL, Y, weights, l2_beta)\n",
    "    \n",
    "    ep = tf.placeholder(tf.float32, name = \"ep\")\n",
    "    learning_rate = starter_learning_rate * (1/(1 + decay_rate*ep))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, beta1 = adam_beta1, \\\n",
    "                                      beta2=adam_beta2).minimize(cost)\n",
    "    \n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        num_minibatches = int(m/minibatch_size) #Number of minibatches in training set\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0\n",
    "            minibatches = random_minibatches(X_train, Y_train, minibatch_size)\n",
    "            \n",
    "            num_minibatches = len(minibatches)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                (X_minibatch, Y_minibatch) = minibatch\n",
    "                _, minibatch_cost = sess.run([optimizer, cost],\\\n",
    "                                             feed_dict={X: X_minibatch, Y: Y_minibatch, ep: epoch, kp: keep_prob})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "            if print_cost_flag and epoch %100 == 0:\n",
    "                print(\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost_flag and epoch %5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "            \n",
    "        if print_cost_flag:\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(\"# Iterations (per 10s)\")\n",
    "            plt.title(\"Learning rate = \"+ str(learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        sess.close()\n",
    "        return parameters\n",
    "\n",
    "\n",
    "def predict_Y_layered_nn(X_input, parameters, activation_fn):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Estimate labels via the L layered NN model framework\n",
    "\n",
    "        Arguments:\n",
    "        X_input -- Input data (normalized) of shape (number of features, sample size)\n",
    "        parameters -- A dictionary containing final calibrated weight and bias variables of all layers\n",
    "        activation_fn -- A hyperparameter that is string used to determine the activation function used in the \n",
    "        hidden layers Currently takes on \"relu\" or \"tanh\"\n",
    "\n",
    "        Returns:\n",
    "        Y_hat_list -- List of predicted labels, where each label value is either equal to zero or one, \n",
    "                      of length=sample size\n",
    "    \"\"\"\n",
    "    (n_x, m) = X_input.shape\n",
    "    Y_hat_list = []\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None))\n",
    "    kp = tf.placeholder(tf.float32, name = \"kp\")\n",
    "\n",
    "    ZL = forward_propagation(X, parameters, activation_fn, kp)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "\n",
    "        epsilon = 1e-10 #Used to avoid a division by zero scenario\n",
    "\n",
    "        Y_hat = sess.run(tf.argmax(ZL), feed_dict={X: X_input, kp: 1.0})\n",
    "        for i in range(m):\n",
    "            Y_hat_list.append(Y_hat[i])\n",
    "    \n",
    "    sess.close()\n",
    "    return Y_hat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.5.3 Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling training, validation, and testing datasets\n",
    "target_label = target_label_master\n",
    "list_of_words = list_of_words_master\n",
    "\n",
    "m_train = len(comments_for_modelling_training)\n",
    "m_validate = len(comments_for_modelling_validation)\n",
    "m_test = len(comments_for_modelling_testing)\n",
    "\n",
    "n_x = np.max(corpus_df[\"order\"])\n",
    "values_y = [0, 1]\n",
    "n_y = len(values_y)\n",
    "\n",
    "X_train = np.zeros((n_x, m_train))\n",
    "Y_train = np.zeros((n_y, m_train))\n",
    "Y_train_list = []\n",
    "\n",
    "X_validate = np.zeros((n_x, m_validate))\n",
    "Y_validate = np.zeros((n_y, m_validate))\n",
    "Y_validate_list = []\n",
    "\n",
    "X_test = np.zeros((n_x, m_test))\n",
    "Y_test = np.zeros((n_y, m_test))\n",
    "Y_test_list = []\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_modelling_training:\n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    Y_train_list.append(y)\n",
    "    \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_train[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_train[j, counter] = word_rep\n",
    "        else:\n",
    "            X_train[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "            \n",
    "    for j in range(n_y):\n",
    "        Y_train[j, counter] = (values_y[j] == y) * 1 #Making Y a one-hot matrix of n_y classes\n",
    "        \n",
    "    counter += 1\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_modelling_validation: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    Y_validate_list.append(y)\n",
    "    \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_validate[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_validate[j, counter] = word_rep\n",
    "        else:\n",
    "            X_validate[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "            \n",
    "    for j in range(n_y):\n",
    "        Y_validate[j, counter] = (values_y[j] == y) * 1 #Making Y a one-hot matrix of n_y classes\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_modelling_testing: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    Y_test_list.append(y)\n",
    "    \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_test[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_test[j, counter] = word_rep\n",
    "        else:\n",
    "            X_test[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "            \n",
    "    for j in range(n_y):\n",
    "        Y_test[j, counter] = (values_y[j] == y) * 1 #Making Y a one-hot matrix of n_y classes\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing inputs\n",
    "mu_X = np.mean(X_train, axis=1).reshape((X_train.shape[0],1))\n",
    "std_X = np.std(X_train, axis=1).reshape((X_train.shape[0],1))\n",
    "epsilon = 1e-10 #Used to avoid a division by zero scenario\n",
    "\n",
    "X_train_n = (X_train - mu_X) / (std_X+epsilon)\n",
    "X_validate_n = (X_validate - mu_X) / (std_X+epsilon)\n",
    "X_test_n = (X_test - mu_X) / (std_X+epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\bksin\\desktop\\jbeast_trader\\env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Model 1 , accuracy_train = 96.32% , accuracy_validate = 89.43% , f1_validate = 57.73% , Time taken = 0.49 minutes\n",
      "Model 2 , accuracy_train = 99.50% , accuracy_validate = 89.95% , f1_validate = 58.95% , Time taken = 0.71 minutes\n",
      "Model 3 , accuracy_train = 97.86% , accuracy_validate = 91.75% , f1_validate = 68.63% , Time taken = 0.32 minutes\n",
      "Model 4 , accuracy_train = 92.83% , accuracy_validate = 91.75% , f1_validate = 62.79% , Time taken = 0.26 minutes\n",
      "Model 5 , accuracy_train = 99.40% , accuracy_validate = 92.27% , f1_validate = 67.39% , Time taken = 0.43 minutes\n",
      "Model 6 , accuracy_train = 95.25% , accuracy_validate = 90.46% , f1_validate = 54.32% , Time taken = 0.37 minutes\n",
      "Model 7 , accuracy_train = 99.59% , accuracy_validate = 92.53% , f1_validate = 69.47% , Time taken = 0.29 minutes\n",
      "Model 8 , accuracy_train = 96.76% , accuracy_validate = 91.75% , f1_validate = 65.22% , Time taken = 0.55 minutes\n",
      "Model 9 , accuracy_train = 92.71% , accuracy_validate = 91.75% , f1_validate = 56.76% , Time taken = 0.31 minutes\n",
      "Model 10 , accuracy_train = 97.96% , accuracy_validate = 90.98% , f1_validate = 63.16% , Time taken = 0.27 minutes\n",
      "Model 11 , accuracy_train = 98.77% , accuracy_validate = 90.98% , f1_validate = 65.35% , Time taken = 0.81 minutes\n",
      "Model 12 , accuracy_train = 90.66% , accuracy_validate = 91.49% , f1_validate = 57.14% , Time taken = 0.37 minutes\n",
      "Model 13 , accuracy_train = 98.02% , accuracy_validate = 93.30% , f1_validate = 69.05% , Time taken = 0.33 minutes\n",
      "Model 14 , accuracy_train = 96.92% , accuracy_validate = 89.43% , f1_validate = 54.95% , Time taken = 0.37 minutes\n",
      "Model 15 , accuracy_train = 97.49% , accuracy_validate = 91.49% , f1_validate = 67.96% , Time taken = 0.57 minutes\n",
      "Model 16 , accuracy_train = 94.69% , accuracy_validate = 91.24% , f1_validate = 60.47% , Time taken = 0.30 minutes\n",
      "Model 17 , accuracy_train = 92.55% , accuracy_validate = 90.72% , f1_validate = 52.63% , Time taken = 0.23 minutes\n",
      "Model 18 , accuracy_train = 94.94% , accuracy_validate = 92.01% , f1_validate = 63.53% , Time taken = 0.25 minutes\n",
      "Model 19 , accuracy_train = 96.38% , accuracy_validate = 91.75% , f1_validate = 64.44% , Time taken = 0.75 minutes\n",
      "Model 20 , accuracy_train = 98.18% , accuracy_validate = 90.98% , f1_validate = 60.67% , Time taken = 0.46 minutes\n",
      "Time taken = 8.42 minutes\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "#Hyper parameter bounds were set by trial and error\n",
    "target_label = target_label_master\n",
    "num_epochs = 1000\n",
    "iterations = 20#Each iteration takes about 0.5 minutes on my laptop (2.6 GH, 16GB RAM)\n",
    "\n",
    "print_cost_flag = False\n",
    "print_performance = False\n",
    "\n",
    "model_list = []\n",
    "f1_validate_best = 0\n",
    "#Calculating base_line_strategy_accuracy\n",
    "if target_label == 1:\n",
    "    baseline_strategy_accuracy_train = 1 - np.sum(Y_train)/Y_train.shape[0]\n",
    "    baseline_strategy_accuracy_validate = 1 - np.sum(Y_validate)/Y_validate.shape[0]\n",
    "    baseline_strategy_accuracy_test = 1 - np.sum(Y_test)/Y_test.shape[0]\n",
    "\n",
    "else:\n",
    "    baseline_strategy_accuracy_train = np.sum(Y_train)/Y_train.shape[0]\n",
    "    baseline_strategy_accuracy_validate = np.sum(Y_validate)/Y_validate.shape[0]\n",
    "    baseline_strategy_accuracy_test = np.sum(Y_test)/Y_test.shape[0]\n",
    "    \n",
    "tic_all = time.clock()\n",
    "for i in range(iterations):\n",
    "    tic_one = time.clock()\n",
    "    \n",
    "    ##Setting hyperparameters\n",
    "    layers_dims = [n_x]\n",
    "    num_layers = np.random.randint(1, 5)#Between 1 and 4 layers --> Low sensitivity hyperparameter\n",
    "    for l in range(num_layers):\n",
    "        num_units = np.random.choice([10, 15, 20, 25]) # --> Medium sensitivity hyperparameter\n",
    "        layers_dims.append(num_units)\n",
    "    layers_dims.append(n_y)\n",
    "    \n",
    "    activation_fn = np.random.choice([\"relu\", \"tanh\"]) # --> Low sensitivity hyperparameter\n",
    "\n",
    "    l2_beta = 10**(-2*np.random.rand() - 2)#Between 1/10**-4 and 1/10**-2 --> High sensitivity hyperparameter\n",
    "    keep_prob = 2**(-np.random.rand()) #Between 0.5 and 1.0 --> Medium sensitivity hyperparameter\n",
    "    starter_learning_rate = 10**(-2*np.random.rand() - 2) #Between 1/10**4 and 1/10**2 --> High sensitivty \n",
    "                                                          #hyperparameter\n",
    "    adam_beta1 = np.random.rand() #Between 0 and 1 --> Low sensitivity hyperparameter\n",
    "    adam_beta2 = np.random.rand() #Between 0 and 1 --> Low sensitivity hyperparameter\n",
    "    decay_rate = 10**(-3*np.random.rand() - 2)#Between 1/10**-5 and 1/10**-2 --> High sensitivity hyperparameter\n",
    "    minibatch_size = np.random.choice([64, 128, 256, 512, 1024, m_train])#--> Medium sensitivity hyperparameter\n",
    "    \n",
    "    parameters = train_layered_nn(X_train_n, Y_train, layers_dims,activation_fn, l2_beta,keep_prob,\\\n",
    "                             starter_learning_rate, adam_beta1, adam_beta2,decay_rate, num_epochs,\\\n",
    "                             minibatch_size, print_cost_flag)\n",
    "    \n",
    "    model_list.append(parameters)\n",
    "    \n",
    "    Y_hat_train_list = predict_Y_layered_nn(X_train_n, parameters, activation_fn)\n",
    "    accuracy_train, precision_train, recall_train, f1_train = assess_model(Y_train_list, Y_hat_train_list, \n",
    "                                                                           target_label, print_performance)\n",
    "    \n",
    "    Y_hat_validate_list = predict_Y_layered_nn(X_validate_n, parameters, activation_fn)                                              \n",
    "    accuracy_validate, precision_validate, recall_validate, f1_validate = assess_model(Y_validate_list,\\\n",
    "                                                                                       Y_hat_validate_list,\\\n",
    "                                                                                       target_label,\\\n",
    "                                                                                       print_performance)\n",
    "    \n",
    "    model = parameters #Adding this here for consistency of notation across model frameworks\n",
    "    if i == 0:\n",
    "        f1_validate_best = f1_validate\n",
    "        best_model = model\n",
    "    else:\n",
    "        if accuracy_train > baseline_strategy_accuracy_train and\\\n",
    "        accuracy_validate> baseline_strategy_accuracy_validate and\\\n",
    "        f1_validate > f1_validate_best:\n",
    "            f1_validate_best = f1_validate\n",
    "            best_model = model\n",
    "            best_activation_fn = activation_fn\n",
    "            \n",
    "    toc_one = time.clock()       \n",
    "    time_taken_one = (toc_one - tic_one) / 60\n",
    "    print(\"Model {}\".format(i+1), \", accuracy_train = {:.2%}\".format(accuracy_train).format(l1_ratio),\\\n",
    "          \", accuracy_validate = {:.2%}\".format(accuracy_validate), \", f1_validate = {:.2%}\".format(f1_validate),\\\n",
    "          \", Time taken = {:.2f} minutes\".format(time_taken_one))\n",
    "    \n",
    "toc_all = time.clock()\n",
    "time_taken_all = (toc_all - tic_all) / 60.0\n",
    "print(\"Time taken = {:.2f} minutes\".format(time_taken_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# true positives =  33\n",
      "# true negatives =  358\n",
      "# false positives =  18\n",
      "# false negatives =  21\n",
      "Baseline strategy 87.44%\n",
      "accuracy = 90.93%\n",
      "precision = 64.71%\n",
      "recall = 61.11%\n",
      "f1 = 62.86%\n"
     ]
    }
   ],
   "source": [
    "#Assessing performance of testing dataset\n",
    "target_label = target_label_master\n",
    "parameters = best_model\n",
    "activation_fn = best_activation_fn\n",
    "Y_hat_test_list = predict_Y_layered_nn(X_test_n, parameters, activation_fn) \n",
    "\n",
    "print_performance = True\n",
    "accuracy_test, precision_test, recall_test, f1_test = assess_model(Y_test_list, Y_hat_test_list, target_label,\\\n",
    "                                                                   print_performance)\n",
    "#Results below show that the testing dataset is a medium to poor performance fit (relative to the other models\n",
    "#explored thus-far):\n",
    "#Accuracy > baseline (but not by much)\n",
    "#f1 is below the 90% desired target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.5.4 Predicting Unlabelled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = target_label_master\n",
    "list_of_words = list_of_words_master\n",
    "\n",
    "m_predict = len(comments_for_prediction_model)\n",
    "n_x = np.max(corpus_df[\"order\"])\n",
    "\n",
    "X_predict = np.zeros((n_x, m_predict))\n",
    "\n",
    "comment_list_predict = []\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_prediction_model:\n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()   \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_predict[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_predict[j, counter] = word_rep\n",
    "        else:\n",
    "            X_predict[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "    comment_list_predict.append(comment)\n",
    "    counter += 1\n",
    "\n",
    "epsilon = 1e-10 #Used to avoid a division by zero scenario\n",
    "X_predict_n = (X_predict - mu_X) / (std_X+epsilon)\n",
    "parameters = best_model\n",
    "activation_fn = best_activation_fn\n",
    "Y_hat_predict_list = predict_Y_layered_nn(X_predict_n, parameters, activation_fn) \n",
    "\n",
    "for comment in comments_for_prediction_deterministic:\n",
    "    y_hat = label_df_labelled[corpus_df[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_list.append(y_hat)\n",
    "\n",
    "if target_label == 1:\n",
    "    p = np.sum(Y_train)/len(Y_train)\n",
    "else:\n",
    "    p = 1 - np.sum(Y_train)/len(Y_train)\n",
    "\n",
    "rand_seed_counter = 0\n",
    "for comment in comments_for_prediction_random:\n",
    "    np.random.seed(rand_seed_counter)\n",
    "    if np.random.rand() < p:\n",
    "        y_hat = target_label\n",
    "    else:\n",
    "        y_hat = 1 - target_label\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_list.append(y_hat)\n",
    "    rand_seed_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments  labels\n",
       "0      4000       0\n",
       "1      4001       0\n",
       "2      4002       0\n",
       "3      4003       0\n",
       "4      4004       0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_candidate_layered_nn = {\"comments\": comment_list_predict, \"labels\": Y_hat_predict_list}\n",
    "predict_candidate_layered_nn_df = pd.DataFrame(data=predict_candidate_layered_nn)\n",
    "predict_candidate_layered_nn_df = predict_candidate_layered_nn_df.sort_values(by=[\"comments\"])\n",
    "predict_candidate_layered_nn_df  = predict_candidate_layered_nn_df.reset_index(drop=True)\n",
    "predict_candidate_layered_nn_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = \"C://Users//bksin//Desktop//NLP//labels_predict_layered_nn.csv\" #Where I stored the files in my computer\n",
    "predict_candidate_layered_nn_df.to_csv(predict_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.5.5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L layered NN approach is a medium to poor fit to the data (relative to the other models explored thus-far). This is likely due to the word padding introduced and the fact that the majority of the comments are below ten words (the longest comment is 190 words suggesting that the padding information can be significantly hindering the neural network's ability to make predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.6 Model Framework 4: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.6.1 Assumptions & Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model framework is the most general to be explored thus far: it account for both word order and varying length. In the literature and in practice, RNNs (with long-short-term-memory, LSTM, layers) are amongst the most popular types of model frameworks used for text classification problems. I use a many-to-one RNN structure with one trainable embedding layer (50 features), one LSTM layer (128 units), and a softmax activation function.\n",
    "Inputs: Each word is converted into an index number from 1 until the maximum number of words in the corpus list of words. Padding will have a zero assigned value.  \n",
    "Output: Estimate of Y, either zero or one\n",
    "\n",
    "Note: A validation dataset will not be used to find the optimal combination of the following hyper parameters for two reason: (1) Preliminary exploration shows that the already default set hyper parameters yields very high performance comparable to the Naive Bayes and Elastic Net models (2) The model is computationally expensive (a downside of its sophistication) to run a decent sample size (e.g., 20) of randomized hyper parameters. Future iterations may incorporate a validation component (possibly a more effective validation optimization approach could be used over a randomized approach that I have been using here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.6.2 Local Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following helper function is based on an assignment completed in the Coursera fifth Deep Learning specialization \n",
    "#course (Sequential models). Modifications were made here to increase flexibility of tweaking hyper parameters\n",
    "#potentially in a future iteration\n",
    "\n",
    "def create_rnn_model_graph(input_shape, vocab_len, emb_vec_len, lstm_dims, keep_prob, n_y):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Create a model graph.\n",
    "\n",
    "        Arguments:\n",
    "        input_shape -- Shape of the input (max_len,)\n",
    "        vocab_len -- Number of words in vocabulary\n",
    "        emb_vec_len -- Hyperparameter representing the length of embedded vectors in the embedding matrix\n",
    "        lstm_dims -- Hyperparameter representing the list of all vector dimensions of each LSTM layers \n",
    "        keep_prob -- Hyperparameter that controls the amount of drop out (a regularization tool)\n",
    "        n_y -- Number of classes of the target value (in this case, n_y = 2)\n",
    "\n",
    "        Returns:\n",
    "        model -- A model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    x = Input(shape = input_shape, dtype = \"int32\")\n",
    "    \n",
    "    AL = Embedding(vocab_len, emb_vec_len, mask_zero = True, trainable = True)(x)\n",
    "    lstm_num = len(lstm_dims)\n",
    "    \n",
    "    for l in range(lstm_num):\n",
    "        d = lstm_dims[l]\n",
    "        if l == lstm_num - 1:\n",
    "            ret_seq = False\n",
    "        else:\n",
    "            ret_seq = True\n",
    "            \n",
    "        AL = LSTM(d, return_sequences=ret_seq)(AL)\n",
    "        AL = Dropout(keep_prob)(AL)\n",
    "\n",
    "    AL = Dense(n_y)(AL)\n",
    "    AL = Activation(\"softmax\")(AL)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=AL)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Y_rnn(X_input, model):\n",
    "    \"\"\"\n",
    "        Objective:\n",
    "        Estimate labels via the RNN model framework\n",
    "\n",
    "        Arguments:\n",
    "        X_input -- Input data of shape (sample size, number of features)##Note that X is transposed in this framework\n",
    "        model -- A Keras object that contains the RNN model specifications\n",
    "\n",
    "        Returns:\n",
    "        Y_hat_list -- List of predicted labels, where each label value is either equal to zero or one, \n",
    "                      of length=sample size\n",
    "    \"\"\"\n",
    "    m = X_input.shape[0]\n",
    "    ZL = model.predict(X_input)\n",
    "    \n",
    "    Y_hat_list = []   \n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat = np.argmax(ZL[i,:])\n",
    "        Y_hat_list.append(y_hat)\n",
    "        \n",
    "    return Y_hat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.6.3 Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling training, validation, and testing datasets\n",
    "target_label = target_label_master\n",
    "list_of_words = list_of_words_master\n",
    "\n",
    "m_train = len(comments_for_modelling_training) + len(comments_for_modelling_validation)\n",
    "m_test = len(comments_for_modelling_testing)\n",
    "\n",
    "n_x = np.max(corpus_df[\"order\"])\n",
    "values_y = [0, 1]\n",
    "n_y = len(values_y)\n",
    "\n",
    "X_train = np.zeros((n_x, m_train))\n",
    "Y_train = np.zeros((n_y, m_train))\n",
    "Y_train_list = []\n",
    "\n",
    "X_test = np.zeros((n_x, m_test))\n",
    "Y_test = np.zeros((n_y, m_test))\n",
    "Y_test_list = []\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_modelling_training.union(comments_for_modelling_validation):\n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    Y_train_list.append(y)\n",
    "    \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_train[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_train[j, counter] = word_rep\n",
    "        else:\n",
    "            X_train[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "            \n",
    "    for j in range(n_y):\n",
    "        Y_train[j, counter] = (values_y[j] == y) * 1 #Making Y a one-hot matrix of n_y classes\n",
    "        \n",
    "    counter += 1\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_modelling_testing: \n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()\n",
    "    y = labels_df_labelled[labels_df_labelled[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    Y_test_list.append(y)\n",
    "    \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_test[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_test[j, counter] = word_rep\n",
    "        else:\n",
    "            X_test[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "            \n",
    "    for j in range(n_y):\n",
    "        Y_test[j, counter] = (values_y[j] == y) * 1 #Making Y a one-hot matrix of n_y classes\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3569/3569 [==============================] - 16s 5ms/step - loss: 0.2510 - acc: 0.9196\n",
      "Epoch 2/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 0.0409 - acc: 0.9913\n",
      "Epoch 3/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 0.0152 - acc: 0.9966\n",
      "Epoch 4/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 0.0051 - acc: 0.9992\n",
      "Epoch 5/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 7/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 5.3400e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 3.2253e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "3569/3569 [==============================] - 15s 4ms/step - loss: 2.4204e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "3569/3569 [==============================] - 16s 4ms/step - loss: 1.5320e-04 - acc: 1.0000\n",
      "Model 1 , accuracy_train = 100.00% , accuracy_validate = 100.00% , f1_validate = 100.00% , Time taken = 2.67 minutes\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "#Parameter settings\n",
    "input_shape = (n_x,)\n",
    "n_y = 2\n",
    "vocab_len = len(list_of_words) + 1\n",
    "\n",
    "#Hyper parameter settings (can be altered in a future iteration)\n",
    "#Hyper parameter bounds were set based on defaults and standard settings (some trial and error was done to check\n",
    "#results)\n",
    "emb_vec_len = 50 # Number of features used in the embedding feature matrix\n",
    "lstm_dims = [128] # Read as One LSTM layer with 128 units\n",
    "keep_prob = 0.5 #Non-drop out probability (used for regularization)\n",
    "epoch_num = 10#Number of epochs. Number is limited as model converges quickly and calibration can be time consuming\n",
    "batch_size = 32 #One mini-batch size\n",
    "learning_rate = 0.001 #standard setting\n",
    "adam_beta1 = 0.9 #standard setting\n",
    "adam_beta2 = 0.999 #standard setting\n",
    "verbose = 1 #This controls the progress display of the model calibration process (set to 0 to remove visibility)\n",
    "\n",
    "print_performance = False\n",
    "\n",
    "tic_one = time.clock()\n",
    "model = create_rnn_model_graph(input_shape, vocab_len, emb_vec_len, lstm_dims, keep_prob, n_y)\n",
    "#model.summary() #Useful for eyeballing model structure and checking total number of parameters (527,606 in this case)\n",
    "adam = optimizers.Adam(lr=learning_rate, beta_1=adam_beta1, beta_2=adam_beta2)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "model.fit(X_train.T, Y_train.T, epochs = epoch_num, batch_size = batch_size, shuffle=True, verbose=1)\n",
    "\n",
    "Y_hat_train_list = predict_Y_rnn(X_train.T, model) \n",
    "accuracy_train, precision_train, recall_train, f1_train = assess_model(Y_train_list, Y_hat_train_list, \n",
    "                                                                       target_label, print_performance)\n",
    "\n",
    "Y_hat_validate_list = predict_Y_rnn(X_validate.T, model) \n",
    "accuracy_validate, precision_validate, recall_validate, f1_validate = assess_model(Y_validate_list,\\\n",
    "                                                                                       Y_hat_validate_list,\\\n",
    "                                                                                       target_label,\\\n",
    "                                                                                       print_performance)\n",
    "toc_one = time.clock()       \n",
    "time_taken_one = (toc_one - tic_one) / 60\n",
    "print(\"Model {}\".format(1), \", accuracy_train = {:.2%}\".format(accuracy_train).format(l1_ratio),\\\n",
    "      \", accuracy_validate = {:.2%}\".format(accuracy_validate), \", f1_validate = {:.2%}\".format(f1_validate),\\\n",
    "      \", Time taken = {:.2f} minutes\".format(time_taken_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# true positives =  52\n",
      "# true negatives =  376\n",
      "# false positives =  0\n",
      "# false negatives =  2\n",
      "Baseline strategy 87.44%\n",
      "accuracy = 99.53%\n",
      "precision = 100.00%\n",
      "recall = 96.30%\n",
      "f1 = 98.11%\n"
     ]
    }
   ],
   "source": [
    "#Assessing performance of testing dataset\n",
    "target_label = target_label_master\n",
    "Y_hat_test_list = predict_Y_rnn(X_test.T, model) \n",
    "\n",
    "print_performance = True\n",
    "\n",
    "accuracy_test, precision_test, recall_test, f1_test = assess_model(Y_test_list, Y_hat_test_list, target_label,\\\n",
    "                                                                   print_performance)\n",
    "#Results below show that the testing dataset is a medium to poor performance fit (relative to the other models\n",
    "#explored thus-far):\n",
    "#Accuracy > baseline (but not by much)\n",
    "#f1 is below the 90% desired target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.6.4 Predicting Unlabelled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = target_label_master\n",
    "list_of_words = list_of_words_master\n",
    "\n",
    "m_predict = len(comments_for_prediction_model)\n",
    "n_x = np.max(corpus_df[\"order\"])\n",
    "\n",
    "X_predict = np.zeros((n_x, m_predict))\n",
    "\n",
    "comment_list_predict = []\n",
    "\n",
    "counter = 0\n",
    "for comment in comments_for_prediction_model:\n",
    "    x = corpus_df[corpus_df[\"comment\"] == comment][\"word\"].tolist()   \n",
    "    n = len(x)\n",
    "    for j in range(n_x):\n",
    "        if j < n:\n",
    "            word = x[j]\n",
    "            if word == \"\":\n",
    "                X_predict[j, counter] = -1 #Assigning -1 for blank words\n",
    "            else:\n",
    "                word_rep = list_of_words.index(word) + 1\n",
    "                X_predict[j, counter] = word_rep\n",
    "        else:\n",
    "            X_predict[j, counter] = 0 #Assigning 0 for end of sentence padding\n",
    "    comment_list_predict.append(comment)\n",
    "    counter += 1\n",
    "\n",
    "Y_hat_predict_list = predict_Y_rnn(X_predict.T, model) \n",
    "\n",
    "for comment in comments_for_prediction_deterministic:\n",
    "    y_hat = label_df_labelled[corpus_df[\"comment\"] == comment][\"label\"].tolist()[0]\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_list.append(y_hat)\n",
    "\n",
    "if target_label == 1:\n",
    "    p = np.sum(Y_train)/len(Y_train)\n",
    "else:\n",
    "    p = 1 - np.sum(Y_train)/len(Y_train)\n",
    "\n",
    "rand_seed_counter = 0\n",
    "for comment in comments_for_prediction_random:\n",
    "    np.random.seed(rand_seed_counter)\n",
    "    if np.random.rand() < p:\n",
    "        y_hat = target_label\n",
    "    else:\n",
    "        y_hat = 1 - target_label\n",
    "    comment_list_predict.append(comment)\n",
    "    Y_hat_predict_list.append(y_hat)\n",
    "    rand_seed_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments  labels\n",
       "0      4000       0\n",
       "1      4001       0\n",
       "2      4002       0\n",
       "3      4003       0\n",
       "4      4004       0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_candidate_rnn = {\"comments\": comment_list_predict, \"labels\": Y_hat_predict_list}\n",
    "predict_candidate_rnn_df = pd.DataFrame(data=predict_candidate_rnn)\n",
    "predict_candidate_rnn_df = predict_candidate_rnn_df.sort_values(by=[\"comments\"])\n",
    "predict_candidate_rnn_df  = predict_candidate_rnn_df.reset_index(drop=True)\n",
    "predict_candidate_rnn_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = \"C://Users//bksin//Desktop//NLP//labels_predict_rnn.csv\" #Where I stored the files in my computer\n",
    "predict_candidate_rnn_df.to_csv(predict_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.6.5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN approach is the best fit discovered thus far with an almost 2% lead over Naive Bayes. Further iteration and exploration may result in an RNN approach besting Naive Bayes. However, the question would come to materiality and computational cost affordability. Should this project be operationalized and use a much larger dataset, simplicity would likely trump over marginal improvement. Nonetheless, I would like to note that a 2% f1 score improvement for a sample of 1,571 is not as meaningful as a 2% f1 score improvement for a sample of 15,710. Thus, the 2% criterion I set here should be revised if we consider different sample sizes (i.e., it should be a function of sample size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Progress Made So Far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3.1 Similarity Analysis of Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes and Elastic Net predictions are 98.09% similar\n",
      "Naive Bayes and RNN predictions are 97.90% similar\n",
      "Elastic Net and RNN predictions are 98.92% similar\n"
     ]
    }
   ],
   "source": [
    "Y_hat_naive_bayes = predict_candidate_naive_bayes_df[\"labels\"].tolist()\n",
    "Y_hat_e = predict_candidate_e_df[\"labels\"].tolist()\n",
    "Y_hat_rnn = predict_candidate_rnn_df[\"labels\"].tolist()\n",
    "\n",
    "similarity_nb_e = 0\n",
    "similarity_nb_rnn = 0\n",
    "similarity_e_rnn = 0\n",
    "\n",
    "m = len(Y_hat_naive_bayes)\n",
    "for i in range(m):\n",
    "    if Y_hat_naive_bayes[i] == Y_hat_e[i]:\n",
    "        similarity_nb_e += 1/m\n",
    "    if Y_hat_naive_bayes[i] == Y_hat_rnn[i]:\n",
    "        similarity_nb_rnn += 1/m\n",
    "    if Y_hat_e[i] == Y_hat_rnn[i]:\n",
    "        similarity_e_rnn += 1/m\n",
    "\n",
    "print(\"Naive Bayes and Elastic Net predictions are {:.2%} similar\".format(similarity_nb_e))\n",
    "print(\"Naive Bayes and RNN predictions are {:.2%} similar\".format(similarity_nb_rnn))\n",
    "print(\"Elastic Net and RNN predictions are {:.2%} similar\".format(similarity_e_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3.2 Concluding Remark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four models have been used to train the labelled dataset in order to predict labels for the unlabelled dataset. Three of the four models have provided an excellent fit, two of which use a word count feature matrix, suggesting the importance of the presence of key words in a comment, trumping over word order. Furthermore, the three models make very similar predictions with about a 2% maximum discrepancy between any of them. Given the 1,572 samples (1,571 of which uses a model prediction) I needed to estimate, this means that the maximum discrepancy is on average about 32 comments out of 1,572 (0.02 * 1,572 = 31.44) in prediction. This strongly suggests that the top-performing models have successfully identified the key underlying logic (within a small margin of error) that dictates the labelling of each comment.\n",
    "\n",
    "Future iterations may lead to a gain in accuracy, though it will likely be marginal. A larger sample size may motivate the need to achieve such marginal gains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this project had two main focuses: Provide a predictive model to predict comments with missing labels; provide the reader with a walkthrough of the thought process and steps taken to arrive at a final model. If such a project were to implemented in practice and to be scalable, I recommend the following two areas of improvement over the work done here:\n",
    "\n",
    "1. Conducting further parameter and hyperparameter searches on the model (depending on resources available). This includes improving the hyperparameter optimization process (e.g., via more iterations especially if computational power is available, via a more effective method than randomization such as gradient ascent and coarse-to-fine methods)\n",
    "2. Exploring a larger range of models to determine if any would be predictive (e.g., support vector machines, decision trees, ensemble models such as a random forest). This would provide more options for datasets with different probability distributions, especially if we use different datasets.\n",
    "1. Automating the whole process from start to finish. This includes missing data handling in all possible forms to model selection and model decision.  This will probably imply using .py files instead of a nice WYSIWYG notebook such as this, which will include test files and server parsing subroutines. Such an improvement will lead to scalability and robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Appendix: Acknowledgements & Key References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the work conducted here (coding and testing) is completely my own, there were key individuals who provided me with insights during this journey:\n",
    "1. **Pouria Fewzee**: Thank you very much for pointing out to me the weakness of adding zero paddings to the comments (this explained why my standard neural network did not perform as well as the other models, and allowed me to successfully tweak the RNN to work well) and suggesting to consider trying using elastic net regularization, drilling in me the idea to start simple and then go more complex  \n",
    "2. **Vincent Russo**: Thank you very much for suggesting to me to consider trying a Naive Bayes approach (the winning model!) and encouraging me to think in an Agile fashion\n",
    "\n",
    "Next to key individuals, I also could not have done without the following key references:\n",
    "1. Andrew Ng's deep learning courses [Improving Deep Neural Networks](https://www.coursera.org/learn/deep-neural-network/home/welcome) and [Sequential Models](https://www.coursera.org/learn/nlp-sequence-models/home/welcome): The key concepts and code samples provided in the assignments I completed were key for developing the neural network and RNN models\n",
    "2. Keras and Tensorflow documentation as well as online posts\n",
    "3. University of Washington's [Machine Learning Foundations](https://www.coursera.org/learn/ml-foundations) and [Regression](https://www.coursera.org/learn/ml-regression) courses that taught me the importance of the f1 score in classification and the different techniques of validation\n",
    "4. Wikipedia on how to implement [Naive Bayes classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
